{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summarizer_Engine_Seq2Seq",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indranildchandra/Abstractive-Text-Summarizer/blob/master/src/Summarizer_Engine_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HdMyi2D4-6YC"
      },
      "source": [
        "# Text Summurization using seq2seq lib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IzJv9UUti_og"
      },
      "source": [
        "### Intro\n",
        "This is a modification to https://github.com/dongjun-Lee/text-summarization-tensorflow \n",
        "I am builging it in a notebook envronment to be able to easily integrate with colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6lSefB0pn9Gq"
      },
      "source": [
        "## Helpers (Link Google Drive to Colab Notebook, Upload Dataset directly, Utility methods)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xO0UtYGfCuU-"
      },
      "source": [
        "### Link Google Drive to Colab Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mGpMpizWCwK1",
        "outputId": "7c91b3e5-7f1b-4d1f-d312-d739e50759d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!wget https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta/+build/16812012/+files/google-drive-ocamlfuse_0.7.4-0ubuntu1~ubuntu18.10.1_amd64.deb\n",
        "!dpkg -i google-drive-ocamlfuse_0.7.4-0ubuntu1~ubuntu18.10.1_amd64.deb\n",
        "!apt-get install -f\n",
        "!apt-get -y install -qq fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "--2019-06-14 04:56:57--  https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta/+build/16812012/+files/google-drive-ocamlfuse_0.7.4-0ubuntu1~ubuntu18.10.1_amd64.deb\n",
            "Resolving launchpad.net (launchpad.net)... 91.189.89.222, 91.189.89.223, 2001:67c:1560:8003::8004, ...\n",
            "Connecting to launchpad.net (launchpad.net)|91.189.89.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://launchpadlibrarian.net/424309408/google-drive-ocamlfuse_0.7.4-0ubuntu1~ubuntu18.10.1_amd64.deb [following]\n",
            "--2019-06-14 04:57:02--  https://launchpadlibrarian.net/424309408/google-drive-ocamlfuse_0.7.4-0ubuntu1~ubuntu18.10.1_amd64.deb\n",
            "Resolving launchpadlibrarian.net (launchpadlibrarian.net)... 91.189.89.229, 91.189.89.228, 2001:67c:1560:8003::8008, ...\n",
            "Connecting to launchpadlibrarian.net (launchpadlibrarian.net)|91.189.89.229|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1236332 (1.2M) [application/x-debian-package]\n",
            "Saving to: ‘google-drive-ocamlfuse_0.7.4-0ubuntu1~ubuntu18.10.1_amd64.deb’\n",
            "\n",
            "google-drive-ocamlf 100%[===================>]   1.18M  2.02MB/s    in 0.6s    \n",
            "\n",
            "2019-06-14 04:57:03 (2.02 MB/s) - ‘google-drive-ocamlfuse_0.7.4-0ubuntu1~ubuntu18.10.1_amd64.deb’ saved [1236332/1236332]\n",
            "\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack google-drive-ocamlfuse_0.7.4-0ubuntu1~ubuntu18.10.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.4-0ubuntu1~ubuntu18.10.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.4-0ubuntu1~ubuntu18.10.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "CPU times: user 281 ms, sys: 74.2 ms, total: 355 ms\n",
            "Wall time: 1min 16s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8F5EuIKKBfY",
        "colab_type": "code",
        "outputId": "b37d3d0e-3f33-47d1-cb5b-883c161cf9da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!ls -lrt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1220\n",
            "-rw-r--r-- 1 root root 1236332 May 18 15:04 google-drive-ocamlfuse_0.7.4-0ubuntu1~ubuntu18.10.1_amd64.deb\n",
            "drwxr-xr-x 1 root root    4096 May 31 16:17 sample_data\n",
            "-rw-r--r-- 1 root root    2557 Jun 14 04:57 adc.json\n",
            "drwxr-xr-x 2 root root    4096 Jun 14 04:58 drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYkEELvtOQme",
        "colab_type": "text"
      },
      "source": [
        "### Upload Dataset directly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_wjyNP4OcoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# #upload summarization_dataset.zip file for training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROT0dBmPO4a9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip summarization_dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7l0W6kHOaqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !ls -lahrt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2pNpy7UgCx9m"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X0VWHFAnoDL4"
      },
      "source": [
        "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hf4b16KopBx7",
        "outputId": "61d2c6a4-7fc7-402e-eeaf-6d001de44787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "!pip install gensim\n",
        "!pip install wget\n",
        "  \n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.4)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.165)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.165 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.165)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.165->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.165->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "CPU times: user 823 ms, sys: 153 ms, total: 976 ms\n",
            "Wall time: 10.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RuLz4sHL7I-",
        "colab_type": "code",
        "outputId": "bd0d9cca-fad8-434c-b5e4-832c7d3c3990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "!wget https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\n",
        "!unzip glove.42B.300d.zip\n",
        "!ls -lrt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-14 04:58:36--  https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.42B.300d.zip [following]\n",
            "--2019-06-14 04:58:37--  http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.42B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877802108 (1.7G) [application/zip]\n",
            "Saving to: ‘glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  14.0MB/s    in 2m 21s  \n",
            "\n",
            "2019-06-14 05:00:58 (12.7 MB/s) - ‘glove.42B.300d.zip’ saved [1877802108/1877802108]\n",
            "\n",
            "Archive:  glove.42B.300d.zip\n",
            "  inflating: glove.42B.300d.txt      \n",
            "total 6742280\n",
            "-rw-rw-r-- 1 root root 5025032584 Dec 22  2015 glove.42B.300d.txt\n",
            "-rw-r--r-- 1 root root 1877802108 Dec 22  2015 glove.42B.300d.zip\n",
            "-rw-r--r-- 1 root root    1236332 May 18 15:04 google-drive-ocamlfuse_0.7.4-0ubuntu1~ubuntu18.10.1_amd64.deb\n",
            "drwxr-xr-x 1 root root       4096 May 31 16:17 sample_data\n",
            "-rw-r--r-- 1 root root       2557 Jun 14 04:57 adc.json\n",
            "drwxr-xr-x 2 root root       4096 Jun 14 04:58 drive\n",
            "CPU times: user 1.32 s, sys: 314 ms, total: 1.64 s\n",
            "Wall time: 3min 22s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTr51ekpOfRj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "064714b4-3c3e-4486-c9b8-bc55b6944b4a"
      },
      "source": [
        "!head -n 20 glove.42B.300d.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ", 0.18378 -0.12123 -0.11987 0.015227 -0.19121 -0.066074 -2.9876 0.80795 0.067338 -0.13184 -0.5274 0.44521 0.12982 -0.21823 -0.4508 -0.22478 -0.30766 -0.11137 -0.162 -0.21294 -0.46022 -0.086593 -0.24902 0.46729 -0.6023 -0.44972 0.43946 0.014738 0.27498 -0.078421 0.36009 0.12172 0.4298 -0.055345 0.4495 -0.74444 -0.26702 0.16431 -0.19335 0.13468 0.2887 0.23924 -0.23579 -0.28972 0.20149 0.048135 -0.18322 -0.15492 -0.19255 0.40271 0.16051 0.17721 0.32557 0.011625 -0.42572 0.34205 -0.45865 -0.2486 0.034128 0.03306 -0.057065 0.18136 -0.43638 0.0005709 -0.11935 -0.2195 0.16429 -0.18119 -0.19145 -0.081672 -0.2962 0.25803 0.073848 0.54213 -0.15405 -0.49256 0.091719 0.13329 -0.05253 -0.20518 0.34576 -1.0449 0.072779 -0.0003453 -0.16926 0.051019 -0.14753 0.23848 -0.40749 -0.58278 -0.48695 0.25863 -0.20531 -0.4775 0.40645 -0.038512 -2.403 -0.12421 0.63149 0.089419 0.08557 -0.20757 -0.1617 -0.29506 -0.13948 0.14202 -0.30138 -0.15806 0.52984 0.24229 0.075169 0.13792 0.90416 -0.23647 0.027788 0.099915 0.45422 0.60176 0.25044 0.29142 0.040712 -0.08121 -0.43786 -0.3015 -0.17991 -0.52149 0.029446 -0.23051 0.073955 0.34751 0.07806 0.19801 -0.32246 -0.13827 0.10076 0.56601 0.31925 0.09426 -0.045898 0.78329 0.19997 0.1619 0.41579 -0.31467 -0.036655 -0.11687 -0.17942 0.16246 0.42221 0.19588 -0.025058 -0.018717 -0.17965 0.35635 0.25853 0.13139 0.026784 0.017271 -0.14781 0.30598 -0.033228 0.15521 -0.50574 0.1295 0.14602 -0.35552 -0.43194 -0.1029 0.04736 -0.57903 -0.42488 0.67163 -0.11182 0.29306 -0.0033312 0.13091 -0.086655 0.22618 0.29357 -0.3088 -0.42705 0.3268 0.39254 0.17474 -0.19659 0.35665 0.38025 0.24257 -0.17021 0.097295 0.45248 -0.40589 0.27886 -0.33315 0.37076 0.16742 -0.28582 -0.051604 -0.090346 0.095385 0.26395 -0.30008 -0.63244 0.076666 0.14102 0.88613 -0.053817 0.26223 -0.016005 -0.040608 0.082136 -0.08159 -0.068912 -0.62239 -0.014757 -0.033402 0.25847 -0.28878 -0.27143 -0.23709 -0.11285 0.24828 0.14512 0.3373 -4.1005 -0.075261 0.32638 0.21444 0.37972 0.029263 0.24594 0.42935 0.68689 -0.58112 0.22939 -0.38889 0.41684 0.066217 0.47901 0.27427 0.41645 -0.35492 -0.14413 -0.010046 -0.42024 -0.19382 0.36156 -0.13364 -0.29853 0.47537 -0.26989 -0.083662 -0.0741 0.21815 -0.30678 -0.83499 -0.11287 -0.32612 0.12375 0.35341 -0.32607 0.32853 0.060266 -0.21991 0.35671 0.29546 -0.48159 -0.22347 0.31036 0.22132 -0.20994 -0.085675 -0.26173 -0.10764 -0.14802 0.17573 -0.17804 -0.21765 0.3073 -0.4459 0.03913 -0.22065 0.2214 0.32727 -0.40378 0.33021 -0.13942 -0.41003 -0.17526 0.21852 0.13615 0.10999 -0.33474 -0.046109 0.1078 -0.035657 -0.012921 -0.039038 0.18274 0.14654\n",
            "the -0.20838 -0.14932 -0.017528 -0.028432 -0.060104 -0.2646 -4.1445 0.62932 0.33672 -0.43395 0.39899 -0.19573 0.13977 -0.021519 0.37823 -0.5525 -0.1123 -0.0081443 0.29059 0.066817 0.10465 -0.086943 -0.048983 -0.26757 -0.47038 0.27469 0.069245 -0.027967 -0.19719 0.016749 -0.29681 0.17838 0.058374 -0.24806 0.085846 0.35043 0.049157 -0.16431 0.50012 -0.18053 0.31422 0.10671 0.031852 0.074278 0.27956 0.080317 0.05478 -0.30349 -0.43215 0.32417 0.40856 0.36192 0.13445 -0.12933 0.11331 -0.15755 0.35755 0.30463 -0.098488 0.012032 0.45581 0.37101 0.1427 -0.43329 -0.10869 0.49849 0.54455 0.44352 0.31804 0.022171 -0.41186 -0.025428 0.21062 -0.3583 0.22028 -0.55391 -0.035364 -0.053998 0.32172 -0.51928 -0.27427 -0.45214 -0.329 -0.48519 0.52966 0.0041434 -0.1718 -0.18748 -0.24365 -0.060786 0.050733 -0.21335 0.27627 0.42745 0.011461 -0.29794 -3.2881 -0.39842 0.16796 -0.12894 0.0020005 0.45613 0.15215 0.15364 -0.21281 -0.25339 0.28955 -0.57817 -0.6074 0.10301 0.28324 0.21506 -0.042325 0.40479 -0.20579 -0.011674 0.26092 -0.15402 0.057961 -0.058576 -0.41974 0.52015 0.15074 -0.088039 -0.14446 -0.17074 0.083752 -0.25708 0.16362 0.14795 -0.059821 0.034473 -0.14534 -0.17965 0.076303 0.33354 -0.14434 0.17618 0.45345 0.15262 -0.0751 0.27592 0.081456 0.30738 -0.072327 0.10706 -0.35581 -0.02669 0.61236 0.70829 -0.28945 -0.024637 0.01189 -0.091899 -0.27272 -0.10157 0.44713 0.092418 -0.10711 -0.015552 0.12822 0.22256 -0.069059 0.29927 -0.10913 0.1618 0.14796 0.1136 0.26634 0.010832 0.071946 0.16973 -0.22769 0.322 -0.083748 0.65269 0.068244 -0.32687 0.31782 0.17035 0.79803 -0.19194 -0.16485 -0.32437 0.079105 -0.35672 -0.26786 -0.24786 0.70512 -0.11909 0.16256 -0.43259 -0.050078 0.050232 -0.1145 -0.041885 0.47866 0.012767 0.19642 0.26196 -0.29425 0.089615 -0.17736 -0.22448 0.22624 0.16749 0.05577 0.14399 0.2158 0.33819 0.23459 0.15826 -0.2856 0.24199 0.11018 0.38164 -0.2984 -0.20169 0.2695 0.11186 -0.21006 -0.04207 0.016507 -0.22866 -3.3882 0.29204 -0.088358 -0.014966 -0.25225 -0.11503 0.036337 -0.14817 0.04622 -0.073466 -0.13866 0.23612 0.033882 0.29495 -0.61234 0.20289 -0.42091 0.37767 0.03626 0.21708 0.12561 -0.21682 -0.0037997 -0.17791 -0.26431 0.31678 -0.051229 0.049269 -0.12622 -0.10117 0.017246 -0.02195 -0.1982 0.03725 -0.16791 -0.055459 0.5767 0.059123 0.22931 0.064201 0.27424 -0.37129 -0.091375 -0.071342 -0.037218 -0.012668 -0.017976 -0.42622 -0.10095 0.044992 -0.090225 0.22915 0.1861 0.36366 -0.20676 -0.33037 0.47302 0.2338 0.079306 0.21083 0.21013 0.15275 0.080873 -0.33013 -0.17181 -0.07017 -0.041244 -0.46182 0.027903 0.54657 -0.25894 0.39515 0.26144 -0.54066 0.21199 -0.0094357\n",
            ". 0.10876 0.0022438 0.22213 -0.12102 -0.048959 0.018135 -3.8174 -0.032631 -0.62594 -0.51898 -0.35893 0.78871 -0.29777 0.005186 -0.41814 0.24691 -0.13584 -0.19046 -0.29987 -0.63696 -0.065406 0.11436 -0.021438 0.17501 -0.19494 -0.089508 -0.10631 -0.24107 0.12856 0.33577 0.051281 0.13849 0.24562 -0.40412 -0.16927 0.22167 0.24016 -0.032447 0.11127 -0.01178 -0.19622 -0.59113 -0.13265 -0.21192 0.17192 -0.18961 -0.34916 -0.098324 -0.42555 0.026523 -0.41361 -0.21346 -0.19775 0.0093893 -0.25976 -0.48221 -0.10778 0.029514 0.29254 -0.14844 0.73861 -0.10725 0.13759 0.54075 -0.1347 0.12982 0.036717 0.32338 -0.077501 -0.04642 -0.23924 -0.25071 -0.059339 -0.40909 0.022929 -0.36144 0.016797 -0.0070788 -0.15447 -0.36141 -0.22258 -0.44854 0.070369 0.19246 0.38875 0.10956 0.20907 0.60839 0.060436 0.042908 -0.13592 -0.1407 -0.56612 0.048278 -0.23504 -0.27897 -2.2884 0.11096 -0.19271 0.14455 0.28472 -0.20574 -0.036538 0.35877 0.16868 -0.063331 -0.18492 0.28085 0.85193 0.28055 -0.20985 -0.19593 0.81482 0.25215 -0.62068 0.41435 0.10557 -0.3093 -0.096312 0.13028 0.15084 -0.50123 0.15623 -0.20892 -0.28956 0.012043 -0.097626 -0.26242 0.096474 0.25771 -0.10903 -0.10082 -0.14434 -0.32984 -0.046379 0.031062 0.014825 0.091645 -0.067844 0.046894 0.15272 -0.088693 0.19054 0.67088 -0.059463 -0.21187 0.16674 -0.23207 0.33023 -0.23966 -0.48016 -0.18978 0.51637 0.13811 -0.26918 0.089576 -0.54118 0.53692 -0.18972 0.046074 -0.27932 -0.10529 -0.73563 -0.087796 0.39539 -0.44176 -0.62707 -0.21666 0.36325 -0.38248 0.49247 0.066497 0.016996 0.17387 -0.30563 -0.036247 0.094577 -0.40638 -0.11703 -0.044589 -0.41872 0.55478 -0.050797 -0.51106 0.081849 -0.23704 0.12522 0.56392 0.22459 0.5077 0.12011 -0.18115 -0.11125 -0.060846 -0.582 -0.32602 -0.061308 0.32683 0.099323 0.2721 0.32524 0.021033 -0.047185 0.51143 0.3579 0.35536 0.005939 0.49009 0.056522 -0.1218 0.021951 -0.21458 0.41146 -0.086966 0.42673 -0.18731 -0.29233 -0.019558 -0.5649 -0.25493 0.12111 0.10869 -0.16795 -0.010423 -3.6964 -0.22244 -0.16492 0.12317 -0.010062 -0.53517 0.30557 0.03702 0.4443 0.015176 -0.26279 -0.065307 0.33099 0.35048 -0.3176 -0.2243 -0.3004 0.24022 -0.18502 -0.66931 0.14986 -0.15948 0.141 -0.047219 -0.083048 0.16829 0.044241 0.35777 -0.22207 0.067496 0.24111 -0.20181 0.64122 0.05656 -0.32635 0.28111 -0.15638 -0.045039 0.14662 -0.41362 0.29122 0.48036 -0.65526 0.041017 -0.40086 0.11574 0.02623 0.028116 -0.4088 0.17148 -0.50448 0.10087 0.3922 0.14905 0.20962 -0.11509 0.32092 -0.32045 0.035085 0.67025 -0.61808 -0.13169 -0.032892 0.21476 -0.0033323 0.52406 0.063131 -0.2123 -0.30088 -0.45161 0.2648 0.075971 -0.40688 -0.29696 0.15939 -0.14902\n",
            "and -0.09611 -0.25788 -0.3586 -0.32887 0.5795 -0.51774 -4.1582 -0.11371 -0.10848 -0.48885 0.19931 -0.1054 -0.43825 -0.34483 -0.45052 -0.34864 -0.458 -0.81554 0.22006 0.20254 -0.10954 0.1252 -0.54117 0.34731 -0.099998 -0.018998 -0.14277 -0.42481 -0.0094091 -0.43155 -0.038769 0.12147 0.51988 -0.4984 -0.24625 -0.52067 -0.05821 -0.30712 0.25512 0.048033 -0.22313 -0.0069182 0.039824 -0.50088 -0.11972 -0.079045 0.01688 -0.34052 -0.2066 0.081265 0.12352 -0.49007 0.34946 -0.29241 0.14893 0.1366 -0.09783 -0.068472 -0.010913 0.0028454 -0.12656 0.3427 0.1058 -0.46151 0.070133 -0.061343 -0.015021 0.17659 0.17941 -0.51377 -0.31381 -0.1372 0.045186 -0.082259 0.21515 -0.21955 0.10313 -0.20704 0.14041 -0.35151 0.62316 -0.5799 -0.056115 -0.0021746 0.18958 0.22398 0.12246 -0.26178 0.010779 -0.31268 -0.21447 0.35344 -0.026041 0.018232 0.35751 -0.070188 -3.0872 -0.13131 0.017387 0.23244 -0.060585 0.20679 0.57579 0.36338 -0.41574 0.030607 0.23619 -0.11284 -0.36043 0.21635 -0.02752 0.17502 0.43491 -0.088247 0.40754 -0.48551 0.13539 -0.090759 0.14423 0.34118 -0.3794 -0.27344 0.02593 0.073217 -0.10176 0.16551 -0.23278 -0.18563 0.021372 -0.093111 0.15179 0.15057 0.55148 -0.20088 -0.079495 0.22599 0.26243 0.25123 0.60266 -0.20423 0.36972 -0.10694 0.0072887 0.018359 0.22368 -0.14065 0.1112 0.087667 0.8466 0.31545 -0.15348 0.020311 0.020878 0.38651 0.047422 -0.24854 -0.19053 0.49173 0.038161 -0.021038 0.14496 0.11591 -0.15105 -0.18942 0.18703 0.026752 0.0046523 0.39814 -0.018617 -0.73177 0.072832 0.41535 -0.48818 0.00304 -0.22729 0.88248 -0.61612 -0.18901 -0.33491 -0.28672 -0.013143 -0.37545 -0.18443 -0.55218 0.70186 -0.073107 0.6393 0.13098 0.071586 0.0053641 0.24636 -0.70744 -0.45036 0.00060187 -0.39093 -0.02716 0.25589 -0.17313 0.29883 -0.00090947 0.08314 -0.4099 -0.013024 -0.049533 0.3041 0.64302 0.23045 -0.18757 0.037584 -0.26082 0.1753 -0.062815 -0.22569 -0.1213 0.15524 -0.14407 0.088732 0.34674 -0.43494 0.38688 -0.15733 -0.12721 0.30194 0.32034 -3.3264 0.069427 0.13848 -0.058216 -0.027088 0.11028 0.3404 0.18654 0.11522 -0.40381 0.044776 0.15535 0.16247 -0.24051 0.04729 0.03498 -0.075942 0.15598 -0.059873 0.0046743 0.15595 -0.27613 0.13562 0.13485 -0.073724 0.31421 0.031234 -0.23516 0.31005 -0.10375 -0.30783 -0.55327 0.28304 0.081429 0.37778 0.15725 0.011757 0.043006 -0.43423 -0.22718 -0.043292 -0.63617 -0.8939 -0.17406 0.41111 -0.14404 -0.1678 -0.44438 -0.73051 0.10957 0.13122 0.085623 0.12504 -0.40337 0.041765 -0.27574 0.062513 0.051093 0.39926 0.11149 -0.056462 0.26809 -0.39569 0.31033 -0.04975 -0.33139 0.47781 -0.021213 -0.21236 0.42374 0.14083 0.067498 -0.12675 -0.3703 -0.092774 0.39058\n",
            "to -0.24837 -0.45461 0.039227 -0.28422 -0.031852 0.26355 -4.6323 0.01389 -0.53928 -0.084454 0.061556 -0.41552 -0.14599 -0.59321 -0.028738 -0.034991 -0.29698 -0.07985 0.27312 0.2204 -0.089859 0.00088265 -0.41991 -0.12536 -0.054629 0.03055 0.1934 -0.063945 0.027405 0.051193 -0.38656 -0.11085 0.17259 0.29804 -0.35183 0.1315 -0.54006 -0.76677 -0.00055168 0.13076 0.025101 0.62106 -0.24797 -0.3979 -0.36116 -0.51967 0.030138 -0.052436 0.069281 0.035252 -0.21402 0.24836 -0.15693 0.12829 0.35425 -0.1608 -0.005072 -0.30656 -0.29514 -0.13554 -0.14385 -0.40552 0.57233 -0.2767 0.30519 0.15586 0.016086 -0.22009 0.48589 -0.41384 0.20546 0.40491 0.041558 -0.13542 0.22544 -0.23629 0.15193 -0.010859 -0.082662 -0.55484 -0.061584 -0.11112 -0.11982 -0.37064 0.16501 0.44063 -0.33883 -0.57676 0.50847 -0.035707 -0.059233 0.030748 -0.27689 -0.070433 0.027786 -0.59336 -2.822 -0.10052 0.67168 -0.17046 -0.25902 0.27938 0.39992 0.03748 -0.26409 -0.26378 0.20645 0.17564 -0.080807 -0.38376 0.26602 0.36214 -0.095112 0.35199 -0.86994 -0.15747 -0.2255 -0.064948 -0.24845 0.15038 -0.32951 -0.22285 -0.025509 -0.29725 -0.37715 0.089296 -0.034399 0.3364 0.35534 0.38253 0.17646 0.13305 -0.32743 -0.47115 0.24673 -0.15964 0.18212 -0.41241 0.098565 0.38118 0.33043 0.051987 -0.21824 0.22214 -0.05945 -0.063743 0.43723 0.11068 0.47444 0.56891 0.31123 -0.20272 0.080078 -0.43905 -0.12246 -0.025057 -0.057162 0.1425 0.094468 0.12991 0.10444 -0.39447 -0.29337 -0.20466 0.20815 -0.1601 -0.14665 0.54511 0.2974 -0.22959 -0.1705 -0.062371 -0.50399 -0.38 -0.39528 0.57552 -0.46892 -0.43308 0.15018 -0.041179 0.62157 0.019874 -0.11969 -0.25611 0.26602 -0.37383 0.12936 -0.050006 -0.11554 -0.17163 -0.4243 0.19844 0.50611 -0.11093 -0.13939 -0.59377 0.67338 0.38497 0.62604 -0.20128 0.30058 -0.13946 -0.16186 0.12168 -0.01841 0.61356 -0.19887 0.1925 0.0084372 -0.50757 0.35858 -0.49729 -0.44725 0.021423 -0.20769 0.083729 0.22032 0.14404 0.001259 -0.44309 -0.17242 -0.353 -0.29477 0.32898 -3.191 0.3891 0.35654 0.052134 0.20576 -0.088649 0.16398 0.11203 0.2859 0.2894 -0.44349 0.91036 -0.30902 -0.13985 -0.39499 -0.027299 -0.15201 0.084418 -0.37196 0.049827 0.14128 -0.15126 -0.16107 0.0040226 0.16799 -0.25429 -0.15074 -0.57409 -0.15611 0.068407 0.24832 0.16828 0.072764 -0.086728 0.0021982 0.13593 0.70224 -0.45976 -0.24506 -0.33874 -0.10952 0.24698 -0.55919 -0.38866 -0.13372 0.091943 -0.10543 -0.31319 -0.29952 -0.20611 0.17976 0.458 -0.072402 0.16118 -0.41649 -0.30103 0.23234 -0.050139 0.10026 0.38974 -0.061342 0.26626 -0.15671 0.075136 -0.42926 -0.12025 0.082736 -0.62469 0.044267 0.60673 -0.12458 -0.15443 -0.16339 0.053097 0.15458 -0.38053\n",
            "of -0.036429 -0.28592 0.063387 -0.60122 -0.015309 0.073243 -3.8547 0.52809 -0.077859 -0.40756 0.44713 0.0056261 -0.38887 -0.0092043 0.26061 -0.45271 -0.50423 -0.38904 -0.30564 0.42003 0.011568 -0.25619 0.55622 0.17323 -0.30415 0.06224 -0.29075 -0.51046 -0.45783 -0.49763 -0.18727 -0.24962 -0.73865 0.046438 0.11561 -0.031821 -0.26618 -0.41581 -0.071389 -0.2079 -0.17899 0.040931 0.1425 -0.43102 -0.2068 0.2589 -0.36623 -0.22408 -0.45842 0.82779 0.53323 0.213 0.14102 0.4644 -0.19063 0.13789 0.23394 -0.48319 0.020489 0.035165 0.1129 0.31478 -0.11344 -0.12321 -0.071496 0.12934 0.51611 -0.17039 -0.10672 0.69655 -0.48179 0.037786 -0.2011 0.21367 -0.023501 -0.43007 0.45458 -0.18225 0.045348 -0.73977 -0.018974 -1.1612 0.046104 -0.59842 0.48152 0.077035 0.079665 -0.059979 0.28133 -0.19488 0.33882 -0.022075 0.34987 0.43741 -0.20609 -0.27658 -3.4348 -0.25458 -0.011177 0.2142 0.21506 0.78159 -0.0064574 0.052498 -0.53285 -0.32415 0.032445 -0.46545 -0.86625 0.13234 0.22265 0.63226 -0.23883 0.20949 0.0081047 0.25664 0.12726 0.4862 0.63486 -0.11147 -0.22792 0.46799 0.098441 -0.22575 -0.050439 0.26147 -0.35885 0.34768 0.46181 0.6723 -0.0088817 -0.3072 0.032602 -0.18653 0.014603 0.21552 0.021596 0.13545 0.30614 0.020874 0.19128 -0.31014 -0.091298 -0.03491 0.089525 -0.14151 -0.10139 -0.10558 0.43202 0.38602 0.067725 -0.49517 -0.43656 0.53588 -0.42404 -0.073479 0.31768 -0.1902 -0.10048 -0.41895 -0.085678 -0.31469 0.20415 -0.16602 -0.10681 -0.17592 0.0045544 0.1717 0.15179 0.02964 0.31657 0.11772 -0.060541 0.54928 0.24473 0.60053 -0.27232 -0.010884 -0.32954 0.21139 -0.05083 0.033311 -0.28025 -0.46744 0.73776 -0.5466 0.2121 -0.088026 0.18173 -0.26704 -0.14732 0.010112 0.12324 0.19112 0.2311 -0.13961 -0.14684 0.35858 0.59945 0.25938 -0.33926 0.48621 0.19161 -0.37456 -0.064856 0.19457 0.3635 -0.11403 -0.50548 -0.030436 0.44118 0.11703 -0.15215 0.17065 -0.3151 -0.16841 -0.51375 -0.06952 -0.28814 0.32903 -0.033493 -0.046118 -0.21614 -0.054066 -3.2795 -0.14166 -0.10367 -0.042882 -0.052912 -0.156 0.20207 0.24538 0.0051342 0.067185 0.08875 0.14647 0.24244 -0.25878 -0.22634 -0.07779 -0.41721 0.0017983 -0.078582 -0.1313 0.10876 0.152 -0.41862 -0.18904 -0.33577 -0.022991 -0.19204 0.0017913 -0.22954 0.28383 -0.64747 0.12466 -0.019468 -0.6369 -0.2137 0.51779 0.23749 0.55759 -0.12286 0.20406 0.26386 -0.094897 -0.20743 0.14492 0.24069 0.52458 0.11511 0.044618 -0.57734 0.32207 -0.097548 0.21445 0.16075 0.16482 0.29271 -0.08841 0.67936 0.32115 -0.21867 0.26709 -0.15852 -0.017671 -0.0092939 0.17614 -0.10259 0.033329 -0.021308 -0.66019 0.090383 0.12859 0.22616 0.30235 -0.25703 -0.48926 0.30537 0.27273\n",
            "a -0.035835 0.77844 -0.51806 0.080682 -0.13173 -0.28606 -4.2485 0.81827 0.24034 -0.69057 0.21556 -0.12434 -0.54229 -0.15138 -0.13591 -0.5348 0.34595 -0.22926 -0.013789 0.047816 -0.041427 -0.41058 0.10172 -0.097093 -0.00064646 -0.026877 -0.23473 -0.06019 -0.3141 -0.5624 -0.079395 -0.22116 0.098952 0.089909 -0.068699 0.30787 0.35724 0.27748 0.21167 -0.020939 -0.48188 -0.14981 -0.19693 -0.19514 -0.029672 -0.2862 0.059519 -0.15534 -0.041982 0.31305 -0.10834 0.74492 -0.26733 0.090418 0.32783 -0.35072 0.0019352 -0.16018 0.35431 -0.066866 -0.094903 -0.026866 -0.44347 0.13844 0.14952 0.34483 0.12546 0.4631 -0.0058689 -0.41446 0.12612 0.13602 -0.43715 0.096268 -0.18979 -0.75418 0.35777 -0.017479 -0.21907 -0.18382 -0.27002 -0.54582 -0.45421 -0.54994 -0.43079 -0.11863 -0.47369 -0.099825 -0.14261 -0.081525 0.33486 -0.4146 -0.079636 0.30339 -0.42575 -0.34956 -2.7357 -0.74104 0.2163 0.42723 0.035431 -0.05464 0.70669 -0.29739 0.0083191 -0.19281 0.32391 0.0087548 -0.16015 -0.059082 0.17503 0.35089 -0.01354 -0.33457 -0.051774 0.01569 1.029 0.044273 0.44906 -0.29036 -0.78684 -0.060001 0.11784 -0.1194 0.20565 -0.30654 -0.10883 -0.076856 -0.027208 -0.0032139 -0.19201 0.13827 0.15239 -0.085574 0.25771 0.035583 0.25709 -0.13264 0.64585 0.539 0.09338 0.17612 0.45444 -0.30252 -0.51751 -0.0077072 0.52078 -0.22996 0.51313 0.6185 -0.271 0.18719 0.3466 -0.27276 0.34786 -0.31713 0.13038 0.021235 -0.25821 -0.14553 0.65942 -0.19469 -0.17672 -0.17879 -0.29119 0.2739 -0.074949 -0.35214 0.12527 -0.1515 0.16561 -0.35536 0.26169 -0.66952 -0.037627 0.24556 -0.47338 -0.19609 -0.16514 0.18519 0.74366 0.099546 -0.30843 0.082241 -0.14698 -0.53421 -0.0061243 0.57767 0.34576 -0.16245 0.062227 -0.274 -0.10936 0.17974 0.32881 -0.0009029 0.072287 0.11748 0.25142 0.062339 0.44903 -0.00094289 0.18164 -0.29528 0.17274 0.41538 -0.1912 0.14755 0.39594 -0.16013 0.064232 -0.32972 0.2003 0.31493 -0.43744 -0.13939 -0.10015 -0.20848 -0.22608 -0.048169 0.20638 0.51249 -0.341 -0.004226 -3.4638 -0.12878 -0.47151 -0.42011 -0.04058 0.042346 0.11615 -0.041935 0.038201 -0.24746 -0.093521 0.44521 -0.044714 0.32429 -0.13014 -0.3915 -0.461 -0.070122 -0.35223 0.38686 -0.25869 0.47497 0.076472 0.0085433 -0.4047 0.66155 -0.23044 -0.021724 -0.021885 -0.00054206 0.0065346 -0.074982 -0.59198 -0.61204 -0.14 0.19463 -0.17722 -0.51057 0.40299 0.2675 -0.29331 -0.18907 -0.37621 -0.4179 0.54875 0.070255 0.80564 -0.3841 -0.42401 -0.41384 0.43875 -0.29252 -0.091183 0.22039 -0.18372 -0.41012 0.62847 0.21983 -0.079124 0.019266 0.85543 -0.13378 0.60141 0.67718 -0.33309 -0.2561 0.084727 0.10459 -0.25359 -0.12002 -0.38965 -0.2878 0.0036703 0.022321 -0.31591 -0.35608\n",
            "in 0.068507 -0.023344 0.28271 -0.40215 0.077815 -0.027003 -3.7645 0.41708 0.058373 -0.067997 0.32045 0.093317 0.35446 0.24415 -0.22822 0.21088 0.092499 -0.0766 -0.0030272 0.65657 0.10773 -0.17828 -0.1082 0.37832 -0.141 0.10373 0.15094 -0.43425 0.00019809 -0.39078 -0.51362 0.09536 -0.42444 0.15611 -0.32383 -0.12708 0.1385 0.21015 -0.3378 -0.4188 0.13787 0.29742 -0.27689 0.04996 -0.54465 -0.20988 0.25044 -0.33203 -0.2728 0.14554 0.60364 0.65656 0.0074453 -0.058784 -0.064545 0.75666 0.087701 -0.1936 -0.34405 -0.1472 0.0067563 0.18505 0.19937 -0.29199 -0.23206 -0.061202 0.21897 0.61196 0.5289 -0.10914 -0.77609 0.44075 -0.63157 0.35396 -0.29959 -0.15911 0.34515 -0.4131 0.41821 0.036273 -0.28904 -1.3799 -0.26149 -0.38145 -0.11302 0.20644 0.19689 -0.16254 0.61176 0.15381 -0.53395 -0.48077 -0.024011 -0.094099 0.22243 -0.14617 -2.9577 0.32627 0.10761 0.25086 0.20389 0.043559 0.37386 0.26832 -0.22787 0.14908 0.14154 -0.10015 0.31386 -0.1703 -0.060314 0.44677 0.64932 -0.3216 -0.55059 0.34674 0.22071 -0.35157 0.57815 0.25799 -0.41677 -0.17724 0.00032002 -0.61324 0.36185 0.055766 -0.136 -0.11536 -0.18133 -0.063293 0.37101 0.41068 -0.45675 0.0019741 0.052907 0.077341 0.047259 0.35521 0.94395 0.44756 -0.23055 -0.29585 -0.24687 0.3778 -0.070669 0.18335 -0.20146 -0.2345 0.60466 0.48813 -0.074877 -0.37743 0.13481 0.10849 -0.016328 -0.56793 0.096551 -0.28974 -0.85841 0.020689 0.55518 -0.20981 0.2382 -0.20757 -0.4081 0.10346 -0.027219 0.23233 0.047661 -0.026813 0.042456 -0.098405 0.40697 0.18637 -0.021596 0.38216 0.033878 -0.2535 -0.29586 -0.019435 -0.73558 -0.2914 -0.50521 -0.47852 -0.22671 -0.26419 -0.11708 0.16524 0.27825 -0.14631 0.051184 -0.009314 0.18186 -0.19979 0.13183 -0.11793 0.27146 0.19013 0.36612 -0.12877 -0.12857 0.094955 0.028475 -0.062946 0.25311 0.15169 0.15963 -0.22283 0.22304 -0.13993 0.23218 0.039904 0.046954 0.3138 -0.26845 -0.26303 0.14821 0.42341 0.34001 0.60787 -0.28699 -0.056269 0.11356 0.029713 -3.479 0.29802 0.03178 -0.035681 0.17751 0.16752 -0.2961 0.044736 -0.036076 0.30142 -0.34389 0.27084 -0.37026 0.2625 -0.47872 -0.0061819 0.055592 -0.061268 0.042886 0.1868 -0.12602 -0.078149 0.33139 -0.26079 -0.25363 0.40634 -0.34571 0.034084 0.1397 -0.1017 -0.11649 -0.19904 -0.027695 -0.31638 -0.038923 0.66143 0.34892 -0.18563 -0.12017 -0.42968 0.56684 -0.46318 0.010384 -0.16257 0.57057 -0.048416 -0.56498 -0.40066 0.17318 0.26049 0.45737 -0.51844 -0.46726 0.012543 -0.3206 0.068685 0.55413 -0.086315 -0.21519 0.66634 0.017061 -0.2999 -0.19917 0.17682 0.15799 0.15006 0.095986 -0.67037 0.59937 -0.14699 0.11697 -0.35628 0.11224 -0.23447 0.4055 -0.28067\n",
            "\" -0.048032 -0.6914 -0.21957 0.089568 0.28472 -1.0531 -3.4121 0.02696 -0.20037 -0.53091 0.97056 -0.034644 -0.10033 -0.2071 -0.42032 0.16462 0.059077 -0.61837 0.23765 0.22807 -0.083801 -0.11536 0.27572 0.26371 -0.17288 0.0084245 -0.51929 -0.36559 0.14597 -0.81487 -0.51025 0.14477 0.46289 -0.25076 0.70546 0.57385 -0.44765 -0.090041 0.16833 0.38911 0.24072 -0.013165 0.074562 0.34116 0.34232 -0.28445 -0.13614 0.070661 -0.6707 0.19258 -0.55693 0.4442 -0.40277 0.04553 -0.010283 -0.040729 0.042784 0.85237 0.46238 0.31426 0.36427 -0.43114 -0.37496 -0.16093 -0.32705 0.53713 -0.25947 0.096877 0.10915 -0.085802 -0.39206 -0.40433 0.25685 -0.53044 -0.52574 -0.34065 -0.25792 0.30916 0.15165 -0.14527 0.040906 -1.0451 3.0427e-05 -0.10925 -0.22796 0.58295 0.17161 -0.50492 0.046715 -0.2565 -0.8094 0.12377 -0.17735 0.064325 -0.10278 0.20066 -2.6094 -0.13329 0.52934 -0.2907 0.039797 0.411 -0.28274 0.14114 -0.149 0.29174 -0.47222 0.27153 0.33043 -0.8398 -0.21368 0.068345 -0.41806 -0.1394 -0.29861 0.3712 0.30367 -0.16679 0.43494 0.39441 0.23276 -0.56044 0.23202 -0.050038 0.025742 -0.037166 0.31038 -0.37319 -0.071117 -0.16484 -0.034022 0.29113 0.41152 -0.04297 0.19572 0.18262 0.086992 0.38578 -0.80169 1.1235 -0.19665 0.252 0.51106 0.50104 0.30746 0.22131 1.0419 -0.12934 0.48314 -0.1097 -0.085398 0.057882 0.056915 -0.19651 0.13835 0.76558 -0.7776 0.14955 -0.0085004 0.10597 0.098488 -0.28773 -0.37583 -0.0099517 -0.49475 -0.027932 -0.15144 0.12913 0.16105 -0.059968 -0.21213 0.27882 0.75464 0.015144 0.13686 -0.18702 0.60299 -0.045254 -0.20525 0.15743 -0.46815 0.048959 0.3447 -0.37844 0.34106 -0.68152 0.11599 -0.15878 -0.044058 -0.18173 0.31114 -0.93244 0.25892 0.2941 -0.48729 0.11488 0.20706 0.71376 -0.0026516 -0.32355 -0.80349 0.32074 -0.10306 -0.038874 0.20804 0.069746 0.10707 0.13026 0.92798 -1.0789 0.59913 0.054721 -0.20213 0.22438 0.18911 -0.44685 -0.2894 -0.051251 -0.18365 0.15081 -0.59379 -0.029135 -0.20623 0.26559 -3.4438 0.65238 -0.68772 0.41854 -0.33688 0.20371 -0.48257 0.3191 0.050891 -0.13833 -0.40396 -0.25854 -0.082192 0.093764 -0.083204 -0.39219 0.14538 -0.5384 -0.081043 0.40279 -0.36503 0.50993 -0.4367 -0.21211 -0.41628 -0.41527 -0.52174 -0.527 -0.51573 0.046207 0.0099332 -0.37437 0.15547 0.14391 0.37315 0.47392 -0.16113 -0.070403 -0.14696 0.22995 -0.24619 0.14984 1.0486 -0.29904 0.11577 -0.48089 -0.24363 -0.13805 -0.67828 -0.080053 0.2871 0.61649 -0.25851 0.25048 0.0051335 0.040719 0.4885 -0.32601 -0.082974 0.82039 -0.054578 0.25842 0.47645 -0.3331 -0.33535 -0.084167 0.19699 0.26016 -0.59807 0.4776 0.48911 -0.33895 -0.43047 -0.087665 -0.22918 -0.0044677\n",
            "is 0.11396 0.34503 -0.055064 -0.20192 0.20848 -0.09699 -4.0236 0.95046 -0.060803 -0.8326 0.39128 0.080882 0.19049 -0.043595 -0.39001 -0.34955 0.11262 -0.14287 -0.032761 0.067908 0.17553 0.31537 0.28348 0.10421 0.35078 -0.032335 -0.36486 -0.34773 -0.20653 -0.0396 -0.23032 -0.077951 0.23275 -0.30656 -0.42728 0.11155 0.56584 0.18156 0.24105 0.26077 0.10482 -0.17409 -0.27386 -0.19085 0.001336 0.15198 -0.085442 -0.45299 -0.25952 -0.10323 0.16033 0.84934 -0.14611 -0.097733 -0.10756 -0.64231 -0.086497 0.19817 -0.145 0.081023 -0.51695 -0.054831 -0.011848 0.14594 0.27602 0.2617 0.47357 0.080774 0.024029 -0.20884 -0.19015 0.13206 -0.3157 -0.38179 -0.099757 -0.47159 0.38781 -0.15529 0.14584 0.061407 -0.34411 -1.2493 -0.55997 -0.51856 0.25404 0.23224 -0.26082 -0.20301 0.091036 -0.15094 0.054542 0.12856 -0.25977 0.57906 0.43496 0.11506 -2.4913 -0.074039 -0.22596 -0.078059 0.37 0.14267 0.29842 0.25159 0.18929 -0.21655 -0.06477 -0.31406 -0.22593 -0.035533 -0.006801 -0.19788 -0.3362 0.11342 -0.26645 0.2006 0.53312 -0.17071 0.29388 -0.1544 -0.32386 -0.23815 -0.074312 -0.0023955 0.31235 0.023316 0.35249 -0.37121 -0.025591 0.11688 -0.1956 -0.037899 -0.059021 0.12694 0.015057 -0.059807 0.11488 0.1346 0.38488 -0.0070541 0.09581 0.26326 0.31792 -0.020803 -0.57065 0.2096 0.34243 -0.13416 0.15929 0.049954 -0.31797 0.092282 -0.36187 -0.15156 0.20105 -0.49372 -0.24939 0.52548 -0.067412 0.0029721 0.48416 -0.29553 -0.1868 0.057142 -0.038733 0.11757 -0.13336 0.2471 0.72767 -0.3194 0.43514 -0.15875 0.20184 0.093158 -0.19095 0.73837 0.3119 -0.18697 0.11706 -0.19537 0.04848 -0.13815 0.083468 0.10713 0.22465 -0.65526 0.51878 0.24839 0.13339 -0.0076668 -0.21111 0.33498 0.42292 -0.19456 0.565 0.45421 0.16063 -0.25718 0.43721 -0.42303 0.097476 -0.015056 0.064036 -0.087359 -0.0071586 0.38121 0.33028 0.29676 0.23904 -0.14327 -0.1011 -0.16298 0.15252 0.066246 0.087008 0.36414 0.23292 -0.54527 -0.17597 -0.57629 -0.27825 -0.21048 0.31815 -0.11145 -3.3707 -0.57578 -0.31548 -0.21493 0.06938 -0.29779 -0.12619 0.40149 -0.1364 -0.60166 0.013699 -0.14001 0.50957 0.034518 -0.082044 -0.15225 0.054835 -0.30277 -0.22071 0.22622 0.26695 -0.032464 0.055506 -0.071883 -0.12037 0.21999 -0.11196 -0.0095203 -0.054642 0.12505 0.38358 0.027555 0.052245 -0.014396 0.13378 0.1317 0.14486 -0.20949 -0.28146 0.063763 0.098775 0.12514 -0.76178 -0.12258 0.21425 0.021548 0.40072 -0.18433 -0.17931 0.079434 -0.62864 0.16945 0.15069 0.25897 -0.31789 -0.29582 0.81134 -0.45501 -0.2029 0.068279 -0.06824 0.29112 0.32883 0.20016 -0.23233 -0.59309 -0.085202 -0.018587 -0.25809 0.21739 0.21592 0.081633 0.12224 -0.37247 0.2879 -0.18029\n",
            "for -0.23909 -0.64189 -0.58322 -0.54743 0.42386 -0.11755 -4.2169 0.28685 -0.30766 -0.6742 0.03867 0.23048 -0.15794 0.18097 -0.36886 -0.062423 -1.0884 -0.017109 0.43104 0.46903 -0.042397 0.40193 -0.32197 -0.15579 -0.33404 -0.30089 -0.0014094 -0.26075 0.35523 0.022844 -0.55828 0.041856 0.35331 0.077003 0.74886 0.055289 -0.04681 -0.46116 0.017485 0.35907 -0.045523 0.15166 0.12532 -0.071976 -0.25125 -0.1633 0.14621 0.1522 -0.061195 0.055225 -0.20055 0.19768 0.089176 -0.24682 0.39557 -0.32195 -0.22106 0.25966 0.22295 -0.40763 -0.73898 -0.18336 0.35442 0.02436 0.0032621 -0.0059417 0.072444 0.14596 0.18363 -0.36354 -0.042161 0.097704 -0.24172 -0.30751 -0.28351 -0.24189 0.21317 0.039802 -0.128 -0.53251 0.14721 -0.88563 -0.26101 -0.23944 0.06401 0.062381 0.097086 -0.013705 0.21058 0.27487 0.20148 -0.73965 0.12504 -0.27577 0.36771 -0.50487 -2.8465 -0.67549 -0.60059 0.046393 0.23611 0.094867 0.73862 -0.273 -0.11245 0.12585 0.41927 0.018334 0.56802 0.062212 -0.79614 -0.14668 0.25178 0.32088 -0.25628 -0.28704 -0.10473 -0.071085 -0.17309 0.12837 -0.51097 0.062379 0.0012732 -0.66104 0.40452 -0.33419 -0.56605 -0.20469 0.50518 -0.31826 -0.13561 -0.26213 -0.33163 -0.6108 -0.11304 -0.018907 -0.19176 -0.21306 0.36105 -0.031309 0.077629 0.6058 -0.34148 -0.13582 -0.082753 -0.071951 0.26333 -0.33163 0.43879 0.078885 -0.026479 -0.18038 0.099418 -0.065447 0.44726 -0.62344 0.05556 0.076232 -0.44274 -0.22871 0.37193 -0.3896 0.11575 -0.093648 0.25593 -0.34525 0.027353 0.086035 0.029737 -0.27269 0.0024322 0.24449 0.11212 -0.19905 -0.44804 0.62767 -0.080553 -0.50877 0.50385 0.42253 0.14679 -0.091549 -0.51924 0.099334 0.20448 0.41728 -0.11322 0.070377 0.34712 0.31102 0.077341 -0.034178 0.34331 0.36052 0.14237 -0.68782 -0.11412 0.13119 -0.20565 -0.15498 -0.26617 -0.58145 0.031516 0.05587 -0.21804 -0.15787 0.44752 -0.57015 0.44162 -0.26597 -0.63425 -0.019741 -0.39778 -0.089568 -0.4413 0.0096704 -0.033922 -0.19073 -0.12692 0.32455 -0.039806 0.26027 -0.3225 -0.24355 -3.1913 0.25131 -0.44537 0.26591 -0.0067639 -0.14244 0.16771 0.65056 -0.36999 -0.05739 -0.20538 0.17873 -0.095203 -0.13814 -0.11228 -0.23044 -0.2295 -0.23337 0.19978 0.36084 -0.23445 -0.090508 0.15121 -0.22822 -0.63549 -0.15704 0.1076 -0.09609 0.08585 -0.015386 -0.22488 0.19139 0.26345 -0.28821 0.44005 0.080017 0.50119 -0.35549 -0.23027 0.14742 0.38131 0.25844 -0.12763 0.34826 0.27506 0.48624 0.27677 0.32541 -0.028055 0.46142 0.57238 0.1628 0.18626 -0.2866 0.59975 -0.16204 0.095396 -0.096285 -0.10466 0.15689 0.35278 0.048956 0.15755 0.36086 -0.060806 -0.29091 0.24121 -0.26148 0.05192 0.035589 -0.1557 0.29341 -0.090526 -0.27433 0.019076 0.12692\n",
            ": 0.18524 -0.37887 0.26603 -0.59301 0.65507 -0.088332 -2.511 0.77913 -0.60289 -0.42877 0.16887 0.98559 -0.49095 0.091008 -0.15037 -0.72456 -0.26129 0.52288 -0.048313 -0.65295 0.21471 0.38799 -0.10577 -0.29741 0.28791 -0.078379 0.13846 0.0475 0.27871 0.48087 0.68015 0.34165 0.53321 -0.13269 -0.094117 -0.27177 -0.26199 -0.96608 -0.37116 0.60954 0.28366 0.28325 -0.25089 0.57108 0.074725 0.2098 0.20687 -0.043224 -1.1696 -0.33808 0.34436 -0.036202 -0.85502 0.18101 -0.44146 1.1333 0.44425 0.070052 -0.028137 0.15389 -0.44741 -0.76146 0.025769 0.57776 -0.17777 -0.14727 -0.08749 -0.084997 -0.4259 0.16521 -0.60322 -0.22595 0.25026 -0.35568 0.054064 0.46647 -0.34279 0.44436 0.19123 0.017965 -0.45413 -0.33474 0.4419 0.16496 -0.16889 -0.16248 0.19866 -0.20442 -0.52295 -0.069982 0.038043 -0.55943 0.49931 0.18783 0.41319 -0.16213 -2.6708 -0.21528 0.33097 0.17506 -0.27152 0.17503 0.061198 -0.048763 0.17543 0.2748 0.36292 0.1034 0.06429 0.094507 -0.1908 0.036522 -0.67388 0.042702 -0.7629 0.061249 0.2339 0.63169 0.51758 -0.07616 0.17195 -0.08313 0.12372 -0.082896 0.59135 -0.15591 0.0020922 -0.036031 -0.017047 -0.32047 0.23856 0.25725 -0.44901 0.20282 -0.04172 -0.46867 -0.12855 0.10297 -0.82072 1.3329 -0.42935 0.58551 -0.28192 -0.83001 0.18132 -0.27045 0.50375 0.15416 0.33177 -0.078226 0.17272 -0.17693 0.65241 -0.43936 -0.38217 -0.028491 -0.022308 0.22461 0.16737 -0.42944 -0.042793 -0.51515 0.30534 0.27292 -0.18177 0.063515 -0.60543 0.11819 0.1424 0.013873 -0.48959 0.24334 0.3911 -0.40884 -5.6906e-05 -0.5198 -0.20315 0.21492 0.03981 -0.38968 -1.032 0.64643 0.4533 0.29474 0.13341 0.1104 -0.11518 -0.75806 -0.6809 -0.080872 0.64041 -0.075078 0.84564 0.082781 0.47554 0.42379 -0.18515 0.65767 0.26898 0.19334 -0.14757 0.62914 0.45802 -0.36344 -0.0028786 -1.2593 -0.41381 0.16725 0.16111 -0.12891 0.35258 -0.15861 -0.15312 -0.36222 -0.31103 0.49892 0.79551 -0.37941 0.68283 0.02999 -0.25482 0.26507 -0.16327 0.1135 -3.768 0.31203 0.11916 0.6706 -0.58409 0.074425 0.049166 -0.065358 0.67052 0.42993 0.20407 -0.6565 0.37971 -0.037463 -0.075834 -0.44218 -0.2961 -0.1381 -0.54767 0.080978 0.57841 0.030677 0.34813 -0.28161 0.42822 -0.017404 0.29732 0.38627 -0.42612 0.7725 0.39136 0.2569 -0.38934 0.35734 0.14599 0.065006 -0.13783 0.14222 -0.040797 -0.20202 0.1349 -0.42088 0.45794 0.071163 -0.26991 1.0147 0.50058 0.45612 -0.77709 0.86904 -0.032782 -0.11692 0.30304 -0.51648 0.63102 -0.076382 -0.15936 -0.50472 -0.74426 -0.21099 -0.089563 -0.091409 -0.6027 0.12475 -0.15683 -0.62756 0.77533 1.2472 -0.9117 -0.12674 0.39212 -0.28231 -0.87358 -0.1776 -0.24893 -0.3163\n",
            "i -0.043504 -0.18484 -0.14613 -0.21751 0.2025 0.044053 -4.2823 -0.034931 0.10337 -0.68931 0.078928 -0.19808 0.10099 -0.21902 0.23785 -0.18958 -0.076054 0.080842 0.30931 -0.038854 0.55597 -0.2242 0.3882 0.15026 -0.29439 -0.56333 -0.4194 -0.27953 0.15539 -0.062982 -0.069327 -0.039159 0.40351 -0.021251 0.3225 -0.10849 -0.34141 -0.12177 0.29162 0.089426 0.020347 0.34924 -0.19546 -0.26214 -0.20894 0.010233 -0.072328 -0.42755 -0.038977 -0.31352 -0.10783 -0.3255 0.17601 -0.32614 -0.16987 -0.057348 0.093224 -0.30891 0.46086 0.10613 0.19984 -0.17867 0.127 -0.29539 0.020387 0.42015 -0.098038 -0.11268 -0.37727 -1.055 -0.23141 -0.20308 0.4013 -0.069317 0.25465 -0.33846 0.6358 -0.064021 0.31587 -0.10926 0.29826 -0.6937 -0.52955 -0.14204 -0.12114 -0.1731 0.16079 -0.052081 0.092534 0.020896 0.21948 -0.081917 -0.096628 0.17746 -0.45614 -0.15351 -1.8311 -0.060287 0.066335 0.2843 -0.073177 -0.25603 -0.052033 -0.10733 -0.026687 -0.3235 0.0011775 -0.14627 -0.074643 -0.17963 0.16691 0.00028942 -0.1076 0.40056 -0.20383 1.0869 0.10362 -0.37086 0.24985 0.20921 0.1671 -0.049194 0.15099 0.30347 0.26684 -0.35441 -0.13476 0.16374 -0.08326 -0.043688 -0.31049 -0.0086838 -0.12193 -0.35278 0.16532 -0.067317 0.52239 -0.18702 0.10217 0.8081 0.11712 0.32835 0.12259 -0.24434 0.43636 -0.4618 0.26365 -0.27708 0.26264 0.026084 0.2331 -0.037621 0.01235 -0.36741 -0.061117 0.033803 -0.29691 0.065067 -0.27793 0.066063 0.28317 0.26602 -0.3253 -0.42407 0.12035 0.46093 0.37397 -0.24189 0.23685 -0.50295 -0.2154 0.11192 -0.12174 -0.094199 -0.27255 0.3302 -0.1564 -0.056053 -0.13629 0.058632 0.045838 0.55262 -0.12814 0.078153 -0.30575 -0.25255 0.22152 -0.38837 -0.0060042 0.0013889 0.39918 -0.28278 0.34967 -0.15238 0.044938 0.10729 0.25289 -0.40761 0.2823 -0.31936 0.33705 -0.26131 -0.11741 0.38829 -0.23005 -0.0044883 -0.30692 -0.37239 0.77491 0.33351 0.54766 -0.15144 -0.0043475 -0.43681 -0.051382 0.13585 -0.17978 0.13712 0.055 -0.10995 0.017427 -0.10725 -0.11014 -0.18704 -3.5946 0.11965 0.4042 -0.11872 0.12854 -0.045573 -0.58077 0.12805 0.18949 0.060238 -0.31284 -0.06668 -0.17352 0.49526 -0.31845 0.22007 -0.30396 0.1529 -1.0693 0.49687 0.1404 0.26403 -0.30594 0.15625 -0.045417 0.40266 0.069006 0.077115 0.032278 0.010174 0.46835 -0.075905 0.45815 -0.089478 0.034056 0.075866 -0.51797 -0.44302 0.026851 -0.13001 -0.035663 -0.35556 0.019674 -0.10015 0.077083 -0.46469 -0.060102 -0.31173 -0.24843 -0.30887 0.051811 0.098181 0.35691 0.2406 0.36622 0.046849 1.1107 -0.28632 0.49063 0.40494 -0.0041022 0.19997 -0.02471 0.21743 -0.10444 0.17242 -0.15222 -0.10119 -0.20508 -0.37518 0.21623 0.26467 -0.081656 0.1008 0.1068 0.089065\n",
            ") 0.26667 -0.23015 -0.01352 0.10943 0.71197 -0.090025 -2.5768 0.88659 0.21025 -0.31948 0.58588 0.67079 -0.12653 0.31632 0.12701 0.14976 -0.49463 0.22453 -0.043073 -0.028317 0.50194 0.28411 0.15523 0.078399 0.24434 -0.33035 0.41031 0.3699 -0.61482 -0.074343 -0.62738 -0.010309 -0.95638 -0.32164 0.074342 -0.2452 -0.41428 -0.048367 -0.37011 0.034152 0.75572 -0.26022 -0.32155 -0.022733 0.56022 -0.14057 0.6231 -0.12515 0.46278 0.090383 -0.24517 -0.20448 -0.44479 -0.18852 0.075946 -0.72078 0.35465 0.38543 -0.26633 -0.45355 0.45138 -0.093898 0.02914 0.52585 -0.46625 0.006238 0.078032 0.17684 -0.25892 0.28059 -0.44845 0.19041 0.36967 -0.21003 0.37713 -0.45568 -0.47609 -0.10363 -0.44441 -0.52894 0.68394 -0.66284 0.65255 -0.18063 0.30286 0.088442 -0.022566 -0.12702 -0.19133 0.63179 -0.35099 -0.10213 0.064386 -0.19172 0.27571 0.18844 -2.5759 0.53847 -0.20514 -0.13072 -0.42337 -0.1293 -0.014951 0.082325 0.11717 -0.36795 -0.33252 0.30988 0.23404 0.023456 -0.23246 0.05062 -0.3451 -0.48295 0.18911 0.37081 0.79475 -0.25726 -0.31557 0.88027 0.281 -0.63176 -0.30507 0.17949 -0.030283 0.63066 -0.049994 -0.20822 -0.38529 0.50226 -0.1818 0.28103 -0.46593 0.0099929 0.11186 0.75747 0.33852 -0.55423 -0.19801 1.3019 -0.39106 0.091197 -0.026081 -0.58972 0.083129 -0.68824 0.23169 0.17169 0.045611 -0.16345 -0.73675 -0.6217 0.35162 0.099628 -0.065537 -0.10064 -0.72815 0.011016 0.31963 -0.47494 -0.30881 0.055299 -0.40056 -0.88493 0.40312 -0.17932 -0.053653 -0.16629 -0.90349 -0.1969 -0.2281 -0.59799 -0.21185 -0.5029 0.0038485 0.38916 0.061698 -0.078992 -0.42085 -0.33722 -0.53146 0.076515 -0.0086711 0.1357 -0.19069 0.011674 -0.032666 0.26495 -0.65841 0.36407 -0.25632 -0.57774 0.28258 0.28344 0.018689 -0.59872 -0.22207 0.20856 -0.47788 -0.64584 -0.10071 -0.40016 0.37631 -0.082383 0.31632 0.56173 0.19774 0.47756 -0.12815 -0.16981 -0.19379 0.024901 -0.24717 0.10622 -0.26694 0.20169 -0.22079 -0.56174 -0.37343 -0.22815 0.44388 0.0026693 -0.071211 0.24887 -3.732 0.60711 0.18625 -0.48488 -0.063947 0.18564 -0.018897 -0.073706 0.10377 0.3413 0.38021 -0.23779 0.77126 -0.24749 -0.58308 -0.22443 0.14134 -0.50424 0.037867 0.17546 0.045416 0.07755 0.63273 -0.22014 -0.099165 0.068551 -0.2086 -0.049241 -0.33025 0.62432 -0.59691 -0.0563 -0.47216 -0.0045292 -0.08314 -0.031489 -0.2116 0.27204 -0.31475 0.24339 0.38134 0.11148 0.075824 0.22624 -0.055018 0.41179 -0.16176 0.065073 -0.40046 -0.21377 -0.04112 0.48922 0.17673 -0.30677 0.41533 0.19262 0.17683 -0.28761 -0.53603 0.39015 0.024465 -0.15745 0.88456 0.41616 -0.10685 -0.16308 0.02115 0.083233 -0.77612 -0.12464 0.13537 0.19015 0.1245 -0.31627 -0.086 -0.70107\n",
            "that 0.15805 -0.26233 0.17418 -0.16879 0.13249 0.0012131 -4.7762 0.29278 0.064338 -0.57404 0.0093862 -0.1188 0.027011 0.046369 -0.14497 -0.13829 -0.11145 -0.38801 0.18732 0.16518 0.32432 -0.3162 0.26395 0.11526 -0.13099 -0.25655 -0.45234 -0.25676 0.10703 -0.15271 -0.034337 0.21461 0.074758 -0.027414 0.026487 0.1582 0.41544 -0.2061 0.34509 -0.20208 -0.069525 0.19557 0.16252 -0.5484 0.012072 -0.084814 -0.22903 -0.49882 -0.35563 0.25246 -0.22043 -0.043004 -0.063622 -0.10204 0.063921 -0.45682 0.023694 -0.014695 -0.0066572 0.13108 0.25108 -0.017136 0.37491 -0.0026861 -0.090906 0.43093 0.22089 0.1143 0.36035 -0.29699 -0.063874 0.41058 0.288 -0.0084504 0.089663 -0.091968 0.51278 0.067225 0.15952 -0.095744 -0.030662 -1.1244 -0.69442 -0.2916 0.21185 -0.13466 -0.049059 -0.035291 0.39259 -0.45495 -0.13256 0.10167 -0.03473 0.058264 0.25017 -0.3245 -2.6218 -0.17705 0.14534 -0.13486 0.18264 0.6143 0.2232 -0.01462 -0.02123 -0.050849 0.077923 -0.27431 -0.037998 -0.0010185 0.18766 -0.161 -0.19281 0.09685 -0.23247 0.14421 0.046675 -0.40389 0.070322 0.0554 -0.055992 -0.30379 0.11034 -0.059317 0.0115 -0.25714 0.3384 -0.25285 -0.21051 0.24993 -0.10559 -0.25626 -0.15801 0.010499 -0.050691 0.1028 0.039207 0.090988 0.16751 0.23153 0.097181 -0.020227 0.23843 0.026643 -0.11058 -0.026795 -0.089131 -0.26301 0.52955 -0.053293 -0.11529 0.080244 -0.21195 -0.48817 0.12773 -0.23775 -0.12838 -0.15058 -0.52733 -0.062478 0.28792 0.1104 -0.50092 -0.044995 -0.10616 0.0073671 -0.077167 -0.026344 0.31104 -0.32879 0.28037 0.0069101 -0.38143 0.062239 -0.23076 0.27866 -0.079068 -0.1258 -0.073944 -0.50021 0.21699 0.31619 -0.019891 -0.10223 -0.16547 -0.54121 0.12544 -0.1587 0.28673 0.041828 0.25419 -0.056333 -0.1053 -0.11093 0.37813 0.15853 0.086719 0.19178 0.32095 -0.23377 0.35128 -0.142 -0.32659 -0.12807 -0.15415 0.27225 -0.027 0.061682 0.52501 -0.35955 0.36374 -0.12214 -0.21645 -0.37055 0.10877 0.29179 -0.24575 -0.17361 -0.20279 0.055728 -0.11747 -0.51449 0.59558 -0.2133 -2.9855 0.46936 -0.22036 -0.13701 -0.050621 -0.18788 0.36484 -0.022135 0.11431 -0.10653 -0.25842 -0.32402 -0.029479 -0.3657 -0.066417 0.06741 -0.097227 -0.078551 -0.44374 0.33902 -0.20005 0.074003 -0.20768 0.028683 -0.20104 0.21462 -0.19482 -0.33149 -0.25056 0.25687 0.12435 0.1336 0.12242 -0.11787 0.074399 0.14078 0.066916 -0.035182 -0.083228 0.027568 0.019225 -0.083424 -0.19695 -0.23925 -0.18437 -0.07988 0.29771 -0.30586 -0.13049 -0.29603 0.16735 0.04944 0.024841 0.36674 0.073818 0.0014377 0.58095 0.29855 -0.17781 0.1284 -0.096951 -0.01067 0.16473 -0.17326 -0.23502 -0.077877 0.14973 0.0020913 -0.16267 -0.012839 0.20886 -0.22857 0.22602 -0.31724 0.23749 0.31161\n",
            "( 0.22566 -0.37112 -0.21545 -0.01641 0.33053 -0.15235 -2.7064 0.93984 0.20524 0.14341 0.70853 0.50396 0.10787 -0.15714 0.28867 -0.084845 -0.37772 0.40769 -0.33547 -0.50082 0.51465 0.28739 0.24833 0.34617 0.029698 -0.61897 0.50856 0.20075 -0.57633 -0.35479 -0.58836 0.029269 -0.83585 -0.48034 0.079937 -0.607 -0.47937 -0.21791 -0.28979 0.055326 0.4014 0.027858 -0.28758 -0.25079 0.61711 0.062885 0.66242 0.35049 0.31549 0.36766 -0.059226 0.10647 -0.11032 -0.23799 0.19918 -0.5222 0.22848 0.20725 -0.12297 -0.64342 0.22021 -0.011356 -0.28359 0.37656 -0.019267 0.045466 0.43289 0.73138 -0.18747 0.61342 -0.45276 0.4998 0.67908 -0.0045247 0.7982 -0.24 -0.27308 -0.029924 -0.65473 -0.11705 0.47704 -0.97719 0.43286 -0.05048 -0.060766 -0.065755 -0.081354 -0.2338 -0.52166 0.44514 -0.33812 -0.11598 0.3125 -0.14082 0.2082 0.23779 -1.9048 0.24316 -0.23711 -0.20603 -0.39892 -0.223 0.3588 0.12751 -0.0073575 -0.23636 -0.61311 0.44607 0.29656 0.087632 -0.12924 -0.055266 -0.093017 0.013243 -0.15858 0.24048 0.74807 0.29853 -0.0027507 0.24778 0.26917 -0.47187 -0.30178 0.089973 0.19959 0.71246 -0.25001 -0.30476 -0.42302 0.30042 0.012942 0.11081 -0.10459 -0.024149 -0.04978 0.1043 0.52985 -0.20601 -0.23441 0.98773 -0.61963 0.17015 0.0577 -0.46928 0.22766 -0.66024 0.33885 0.2728 -0.19034 0.048107 -0.90716 -0.56721 0.024759 -0.045172 -0.40045 -0.45319 -0.40496 0.15477 -0.061433 0.01009 -0.47817 0.23192 -0.34431 -0.28817 0.0010155 -0.33073 -0.25678 0.024586 -0.13367 -0.22997 -0.48891 0.031355 -0.34323 0.2834 0.29578 0.52079 -0.078017 -0.72268 -0.56161 -0.25226 0.011491 -0.049597 -0.25661 -0.16012 0.029665 -0.081102 0.20477 0.10081 -0.54743 0.14727 0.11488 -0.16474 0.14744 0.28595 -0.15144 -0.073234 -0.11106 0.061497 -0.41449 -0.81262 0.016904 -0.12148 -0.0054016 -0.14345 0.36979 0.58625 0.31731 0.21677 0.10616 -0.56594 -0.3444 -0.16539 -0.27407 -0.39674 -0.2103 0.087907 -0.034684 -0.68675 -0.056865 0.18112 0.59311 0.26316 -0.2327 -0.035298 -4.0979 0.47216 -0.28693 -0.33641 -0.43626 0.46494 0.01585 -0.049069 0.033508 0.50443 0.46245 -0.32496 0.29819 -0.3999 -0.66735 -0.36449 -0.27742 -0.3471 -0.14502 0.35473 -0.14664 -0.24655 0.27225 -0.33168 0.086345 -0.093201 0.038639 0.21626 -0.60329 0.96447 -0.61508 -0.36709 -0.6391 0.1957 -0.098272 -0.001105 0.10694 -0.085983 0.34202 0.49881 0.43774 -0.1966 -0.13882 0.017656 -0.01865 0.48943 -0.40127 0.21374 -0.3822 -0.22486 -0.21586 0.428 0.69237 -0.39792 -0.10378 0.074862 0.25537 -0.43377 -0.67097 0.676 0.26108 -0.3603 0.67061 0.31876 0.13483 0.12858 0.25021 -0.22768 -0.64548 0.099261 0.081619 -0.2086 -0.33346 -0.37321 -0.12199 -0.83965\n",
            "you -0.045315 -0.010389 -0.19798 0.0074069 0.13915 0.28315 -4.8578 0.059722 -0.041928 -0.67951 -0.22934 -0.037041 0.19498 -0.13721 -0.080921 -0.36824 -0.2795 -0.085557 0.35912 0.059352 0.54356 -0.22451 0.185 0.07815 0.16331 -0.089605 -0.46924 0.013166 -0.23992 0.04183 -0.13411 -0.055497 0.51085 -0.095635 0.0074226 -0.15482 -0.16071 -0.17246 0.040188 -0.12358 0.057126 0.27466 -0.26531 -0.69652 -0.10367 0.36027 -0.22818 -0.14271 0.31034 -0.063622 -0.32047 -0.21441 -0.0066035 -0.44397 -0.23058 -0.46539 0.13063 -0.33903 -0.27848 0.10887 0.23154 -0.11992 0.45113 -0.27517 0.24792 0.56239 -0.11223 0.069393 0.21719 -0.45379 0.072769 0.26735 0.12143 -0.12206 -0.41705 -0.040107 0.64978 0.24525 0.31934 0.12291 0.17259 -0.033497 -0.14209 0.1935 -0.087972 0.035018 0.28217 -0.10064 -0.34261 0.28151 -0.099521 -0.21059 -0.0066425 0.14989 0.04176 -0.79869 -2.4338 0.04226 0.47656 0.14805 -0.09508 0.19522 0.43368 -0.16548 -0.28798 -0.28266 0.16387 -0.4426 -0.42175 -0.11584 -0.019352 -0.3026 -0.095344 0.26175 -0.55522 0.076841 0.21394 -0.17809 0.007211 0.28537 0.010352 0.024934 0.44049 -0.17627 0.2039 -0.44226 0.062315 -0.038854 -0.14574 0.2271 -0.45789 0.11289 -0.084069 -0.049833 0.23436 0.015757 0.59109 -0.48919 -0.24474 0.90254 0.30238 0.17645 0.36994 -0.014467 -0.22783 0.018348 -0.14856 -0.31404 0.1153 -0.10809 -0.027889 0.11631 0.077681 -0.20062 0.18744 -0.042037 -0.10761 -0.027421 0.21262 0.003162 0.58733 -0.084987 -0.47666 0.075267 -0.29273 0.082457 0.078855 0.31808 0.41172 0.14907 -0.14365 0.019729 -0.24756 -0.34937 -0.29368 0.17052 0.16058 -0.27608 0.34404 -0.37101 0.285 -0.13341 -0.17254 -0.25566 -0.042631 -0.34874 0.45017 0.054868 0.31624 -0.29514 -0.16589 0.34528 -0.056808 -0.00018091 0.63327 -0.39434 0.46145 -0.050789 0.22207 -0.046614 0.31343 -0.0868 -0.12464 0.15644 -0.25294 -0.10938 -0.19215 -0.20576 0.38496 -0.24239 -0.027065 -0.44619 -0.018965 -0.17858 0.1412 0.043227 -0.074112 -0.084025 -0.046369 -0.13821 -0.076064 -0.04409 -0.091823 -0.19374 -3.0363 0.23397 -0.28761 -0.031891 0.1153 -0.16362 -0.11093 0.26248 0.10115 -0.0052696 -0.17305 -0.074938 -0.14486 -0.024029 -0.26933 -0.16745 -0.24481 -0.0043717 -0.64694 -0.081254 -0.048565 0.11387 -0.23756 0.43416 -0.087395 0.065356 -0.019127 -0.070495 -0.24685 -0.3798 0.1424 -0.30037 0.17801 -0.02579 -0.086309 -0.16668 -0.28964 -0.41512 0.16919 -0.33368 0.00691 -0.19671 -0.16635 -0.13223 0.33972 -0.09311 -0.26781 -0.35747 -0.47576 0.025746 0.16696 -0.11538 0.0097573 0.33078 0.45239 -0.27258 0.41012 -0.20411 -0.077906 0.26716 -0.22702 0.33231 -0.0090755 -0.047777 -0.27195 -0.13876 0.089044 -0.18412 -0.36055 -0.12008 0.1192 -0.10877 -0.19421 -0.41484 0.11124 -0.12042\n",
            "it 0.39457 -0.013387 -0.23289 -0.1601 0.32788 -0.071888 -4.9166 0.42002 -0.070704 -0.72314 0.091783 -0.30671 0.26245 -0.092867 -0.2409 0.048326 -0.072396 -0.067508 0.21836 -0.040157 0.25487 -0.286 -0.0093754 -0.24727 0.10982 -0.13302 -0.23697 -0.012397 0.15094 0.067324 -0.29848 -0.071428 0.11688 -0.2442 -0.094349 0.2311 0.16316 -0.21772 0.1929 -0.20452 0.20977 0.071821 -0.19373 -0.030947 0.056555 0.10305 0.044956 -0.41205 -0.27989 -0.26539 -0.14482 0.27595 -0.24897 -0.12444 -0.12817 -0.5881 0.031092 -0.19695 -0.059683 0.022946 0.34043 -0.13274 0.10429 -0.034739 0.011724 0.242 0.24059 0.14266 0.18056 -0.19891 0.0040047 0.32522 -0.077831 -0.34488 -0.21437 0.12465 0.38722 0.061659 0.10701 0.19208 -0.44891 -0.61791 -0.33707 -0.13119 -0.091813 -0.09144 -0.094051 0.13404 -0.19336 0.029288 0.14759 0.027914 -0.14386 0.16088 0.12491 -0.063705 -2.1602 -0.24617 0.16771 -0.079031 0.10777 0.27368 0.27345 0.27381 -0.091462 -0.24853 0.26203 -0.214 0.008775 -0.29821 -0.19149 -0.064224 -0.099153 -0.12822 -0.31856 0.26011 0.42875 -0.3095 -0.015697 0.31237 -0.25497 -0.053742 0.27814 0.10271 -0.15525 -0.22801 -0.00011426 -0.3545 -0.016981 0.26014 -0.40949 -0.093622 -0.045268 0.19359 0.20905 0.012217 0.36235 0.1706 0.30244 0.50852 -0.056089 0.26853 0.41433 0.19774 -0.0077277 0.059937 0.49664 -0.28083 0.38949 0.27273 -0.064584 0.14594 -0.16866 -0.51417 0.41891 -0.34648 -0.14804 0.01946 -0.41698 -0.044667 0.27043 0.20731 -0.30694 -0.037941 0.14281 -0.02102 0.024401 0.12598 0.61952 -0.45826 -0.028592 -0.051178 -0.23245 -0.1378 -0.22968 0.17349 -0.18584 0.21256 -0.13043 -0.47431 0.069997 0.13364 0.18428 0.098638 -0.25771 -0.34397 0.13438 -0.25553 0.095159 -0.095461 0.18735 -0.43694 -0.095595 -0.21427 0.19406 -0.032257 -0.012231 -0.12799 0.19049 -0.34733 0.18906 -0.23075 -0.51525 0.12028 -0.027512 0.1711 0.19029 -0.0012909 0.47697 0.1228 0.093411 -0.20419 -0.19148 -0.24021 0.10563 0.057533 0.12664 -0.19644 -0.19929 -0.14172 -0.0019814 -0.5875 0.36457 -0.42278 -3.0764 0.30909 -0.064355 0.03457 0.32523 -0.54723 0.143 0.2029 -0.0016222 -0.32061 -0.046331 -7.4381e-05 -0.064936 0.13024 -0.17944 -0.049575 0.13376 0.060678 -0.68073 0.3987 0.13057 -0.069976 -0.010967 0.063051 -0.033454 0.007602 0.053216 -0.18879 0.0077993 -0.21944 0.24179 0.024485 0.032077 -0.027992 -0.46716 0.13913 0.08745 0.21441 -0.05296 -0.40107 0.0070542 0.1232 -0.29873 -0.11932 -0.074749 0.0039676 0.27828 0.032495 -0.23917 -0.26956 0.0065476 0.045396 0.28008 0.18561 -0.13325 -0.082449 1.0633 -0.36476 0.12634 -0.032615 -0.08159 -0.040808 -0.10173 -0.25261 -0.2811 -0.059487 -0.022828 -0.0058922 -0.22167 -0.13003 0.15824 0.04191 0.36091 -0.17451 -0.092674 0.20148\n",
            "on 0.00060657 0.048631 0.48969 0.42777 -0.3861 -0.0084231 -3.6027 0.47811 0.047945 -0.31859 -0.21335 -0.51531 -0.17142 -0.20035 0.70538 -0.17186 -0.54713 0.68465 -0.040384 0.24141 0.53936 0.10057 -0.14953 -0.28165 -0.44468 -0.69436 -0.10518 -0.30014 0.12637 0.71193 -0.20152 -0.11507 0.053484 0.13611 0.0023964 0.24965 -0.26389 -0.64683 -0.16104 -0.37846 -0.20946 -0.10369 0.22179 -0.72445 -0.055569 -0.51117 -0.020982 0.14127 -0.39321 0.22062 0.082094 0.053219 -0.86973 -0.17208 0.043798 -0.05605 0.36899 -0.092699 0.19369 0.59429 -0.10439 0.52525 0.41624 0.15565 -0.19486 -0.46395 -0.16372 0.32242 0.30328 -0.10038 -0.31126 -0.091417 -0.43295 0.051531 0.059131 0.099654 0.29146 0.019551 -0.22547 -0.31254 -0.43268 -0.59699 0.25903 0.0061463 0.23724 0.17422 -0.62129 0.30664 0.11598 0.16893 0.26485 -0.023562 -0.28476 0.32007 0.33411 -0.79123 -2.5263 0.77887 -0.036362 -0.22488 0.21838 -0.18917 -0.17813 -0.12703 -0.09161 -0.25722 -0.23141 0.24344 -0.29977 -0.39411 -0.31091 -0.71663 0.71927 0.40314 -0.3998 -0.081807 0.48831 -0.26558 -0.2449 -0.72728 -0.65682 0.61916 -0.082432 -0.66695 -0.07183 -0.0060302 -0.14047 0.10774 -0.0294 0.43884 -0.10434 0.77235 0.26651 -0.15147 -0.15951 0.11666 0.033501 0.055742 -0.07072 0.81983 -0.094083 0.17576 0.32675 -0.05292 0.29963 0.062303 0.30784 -0.48853 0.87214 0.48469 0.15472 -0.47794 0.0035701 0.2851 0.45573 -0.49919 -0.32528 0.0050259 -0.10745 -0.3838 0.18128 0.31121 -0.16429 0.25904 -0.0020586 -0.42063 -0.06752 0.11249 -0.062841 0.0090842 -0.62864 -0.23783 0.16221 0.22003 -0.13259 -0.34341 -0.0038345 -0.031361 0.29267 0.011335 -0.16943 0.62525 0.0018924 0.3443 -0.077261 0.090077 -0.29513 -0.3287 0.02348 -0.12653 0.093243 -0.5803 -0.18007 -0.036697 -0.091138 0.080947 -0.13167 0.069933 -0.019786 -0.26451 0.095616 -0.0388 -0.23523 -0.11224 0.29692 -0.13358 -0.018443 -0.55351 -0.071414 0.30206 -0.040702 -0.099249 -0.62182 0.13056 0.37276 -0.51884 -0.72358 0.19346 -0.077715 0.22269 -0.43023 -0.13878 0.45398 0.40267 -3.799 0.29864 -0.10014 0.053288 0.27437 -0.18284 0.59567 0.59842 -0.32466 0.0035302 0.3772 -0.072068 -0.31471 -0.12439 -0.39985 -0.36131 -0.0094553 0.41125 -0.44127 0.13517 -0.18269 -0.47905 -0.056259 -0.23687 -0.20079 0.26212 0.18157 -0.35366 0.23476 0.12185 0.16606 0.3574 -0.45782 -0.35614 0.29046 -0.29956 0.26952 0.01434 -0.026618 0.31496 0.21012 0.103 -0.51883 -0.50328 0.14767 -0.14743 0.2322 0.029708 0.1736 0.39443 0.19012 0.059954 0.40225 -0.13333 0.43591 -0.19416 0.4944 -0.35507 -0.17199 -0.2775 0.026634 -0.16227 0.16202 0.37628 -0.19416 0.21528 0.27162 0.27112 -0.27549 0.28545 0.14925 -0.28046 -0.49446 0.13731 0.13866 0.314\n",
            "- -0.27881 -0.40271 -0.18591 -0.27202 0.28409 0.25364 -2.5526 0.12913 -0.59799 -0.66368 -0.21798 1.1541 -0.28388 0.47289 0.23968 0.16269 -0.19289 0.59471 -0.4954 -0.57984 0.098453 0.50695 0.0076513 -0.096948 0.33477 0.073464 0.081899 -0.16084 0.3847 -0.0033582 -0.43921 -0.16995 0.73148 -0.43821 0.23778 -0.20635 -0.69562 0.81194 0.28341 -0.069185 -0.2758 0.31212 -0.54979 -0.25831 0.43145 -0.51619 -0.051051 -0.58811 0.1407 -0.73657 -0.56138 -0.33934 -0.35815 0.75992 -0.028904 -0.42664 0.09859 0.78329 -0.036797 -0.74718 0.42716 -0.17777 -0.22688 0.06514 -0.1778 -0.065114 0.22365 0.3185 0.041531 0.84846 -0.74444 0.18511 -0.33613 -0.252 -0.48548 -0.29405 0.001448 0.4855 -0.59234 0.32173 0.92677 -0.65704 -0.015583 0.38582 -0.15485 0.42602 0.84531 0.16892 0.041489 0.3778 -0.7439 -0.10633 0.19555 -0.26168 -0.34556 -0.0022175 -2.2717 0.21014 -0.21056 0.17363 -0.78818 0.0061095 0.058268 -0.44657 -0.054329 -0.48545 -0.16541 0.91224 -0.73494 -0.037926 -0.44599 -0.83551 -0.21933 -0.10028 -0.24455 -0.13427 0.3637 0.60357 0.19547 -0.22274 -0.53428 -0.4656 0.091585 0.24125 0.25163 -0.13095 -0.073739 0.62393 -0.14401 -0.55367 -0.50398 0.08876 -0.33294 0.20396 0.59646 -0.80374 0.3395 -0.16926 -0.44729 1.0446 -0.12029 -0.067893 -0.14972 -0.29282 -0.27511 -0.21115 0.44957 -0.32285 0.29852 -0.32795 0.23129 -0.65126 -0.16774 -0.12425 -0.35081 -0.32512 0.70651 -0.15158 0.25328 -0.65039 -0.12799 0.72675 -0.30144 -0.38425 0.39036 -0.046291 0.010203 -0.40669 0.062884 0.11154 -0.53525 -0.30577 -0.051006 0.32127 0.22955 -0.45942 0.10572 -0.35493 0.4441 0.30738 -0.35036 -0.71524 0.10817 -0.14946 0.57782 0.10566 0.44825 0.47138 -0.048961 -0.4194 -0.25798 -0.061018 0.39211 -0.82027 -0.36804 -0.031183 -0.10098 0.071381 -0.12357 -0.33102 -0.32745 -0.36051 0.24736 0.17 0.017463 -0.32852 0.41652 -0.058722 0.22584 0.64524 -0.3729 -0.4406 -0.17576 0.27024 0.19166 0.096023 0.15516 -0.23524 -0.37527 -0.20913 -0.18731 -0.046447 0.17133 0.095358 -4.1563 0.3794 0.12585 -0.055807 0.25315 -0.099114 -0.68619 0.28612 0.3537 0.75654 0.52138 0.28431 -0.26772 -0.16692 0.28497 -0.62988 0.48406 0.20377 -0.11219 0.65251 0.011257 0.056178 0.87677 0.055248 -0.04219 -0.45015 -0.33092 0.28266 -0.15198 0.0058738 -0.067593 -0.20925 0.37602 -0.40575 0.44603 0.14612 -0.10846 0.055436 -0.22304 -0.2564 -0.086234 0.32539 0.23037 -0.47501 -0.20469 0.5957 -0.1538 0.14353 -0.55308 0.1551 -0.26415 -0.057782 0.55282 -0.41416 0.14159 -0.50983 0.28156 -0.70852 0.091054 0.45901 -0.43372 0.15699 0.48727 0.21601 0.21845 0.039081 0.42541 0.24365 0.046829 -0.41852 0.77665 0.37893 -0.0075525 0.3788 0.25532 0.31518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIQ3VwuUOrrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Example to directly generate GloVe vector embeddings from text file\n",
        "# !python -m gensim.scripts.glove2word2vec --input glove.42B.300d.txt --output model_glove_300.vec\n",
        "# !ls -lrt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lA1tCj2mn_2x",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import collections\n",
        "import pickle\n",
        "import numpy as np\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "default_path = \"drive/Colab Notebooks/summarization_dataset/\"\n",
        "\n",
        "train_article_path = default_path + \"train/train.article.txt\"\n",
        "train_title_path   = default_path + \"train/train.title.txt\"\n",
        "valid_article_path = default_path + \"train/valid.article.filter.txt\"\n",
        "valid_title_path   = default_path + \"train/valid.title.filter.txt\"\n",
        "\n",
        "#valid_article_path = default_path + \"DUC2003/input.txt\"\n",
        "#valid_title_path   = default_path + \"DUC2003/task1_ref0.txt\"\n",
        "\n",
        "def clean_str(sentence):\n",
        "    sentence = re.sub(\"[#.]+\", \"#\", sentence)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def get_text_list(data_path, toy):\n",
        "    with open (data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        if not toy:\n",
        "            return [clean_str(x.strip()) for x in f.readlines()][:200000]\n",
        "        else:\n",
        "            return [clean_str(x.strip()) for x in f.readlines()][:50]\n",
        "\n",
        "\n",
        "def build_dict(step, toy=False):\n",
        "    if step == \"train\":\n",
        "        train_article_list = get_text_list(train_article_path, toy)\n",
        "        train_title_list = get_text_list(train_title_path, toy)\n",
        "\n",
        "        words = list()\n",
        "        for sentence in train_article_list + train_title_list:\n",
        "            for word in word_tokenize(sentence):\n",
        "                words.append(word)\n",
        "\n",
        "        word_counter = collections.Counter(words).most_common()\n",
        "        word_dict = dict()\n",
        "        word_dict[\"<padding>\"] = 0\n",
        "        word_dict[\"<unk>\"] = 1\n",
        "        word_dict[\"<s>\"] = 2\n",
        "        word_dict[\"</s>\"] = 3\n",
        "        for word, _ in word_counter:\n",
        "            word_dict[word] = len(word_dict)\n",
        "\n",
        "        with open(\"word_dict.pickle\", \"wb\") as f:\n",
        "            pickle.dump(word_dict, f)\n",
        "\n",
        "    elif step == \"valid\":\n",
        "        with open(\"word_dict.pickle\", \"rb\") as f:\n",
        "            word_dict = pickle.load(f)\n",
        "\n",
        "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
        "\n",
        "    article_max_len = 50\n",
        "    summary_max_len = 15\n",
        "\n",
        "    return word_dict, reversed_dict, article_max_len, summary_max_len\n",
        "\n",
        "\n",
        "def build_dataset(step, word_dict, article_max_len, summary_max_len, toy=False):\n",
        "    if step == \"train\":\n",
        "        article_list = get_text_list(train_article_path, toy)\n",
        "        title_list = get_text_list(train_title_path, toy)\n",
        "    elif step == \"valid\":\n",
        "        article_list = get_text_list(valid_article_path, toy)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    x = [word_tokenize(d) for d in article_list]\n",
        "    x = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in x]\n",
        "    x = [d[:article_max_len] for d in x]\n",
        "    x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in x]\n",
        "    \n",
        "    if step == \"valid\":\n",
        "        return x\n",
        "    else:        \n",
        "        y = [word_tokenize(d) for d in title_list]\n",
        "        y = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in y]\n",
        "        y = [d[:(summary_max_len - 1)] for d in y]\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
        "    inputs = np.array(inputs)\n",
        "    outputs = np.array(outputs)\n",
        "\n",
        "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
        "            yield inputs[start_index:end_index], outputs[start_index:end_index]\n",
        "\n",
        "\n",
        "def get_init_embedding(reversed_dict, embedding_size):\n",
        "    glove_file = \"glove.42B.300d.txt\"\n",
        "    word2vec_file = get_tmpfile(\"word2vec_format.vec\")\n",
        "    glove2word2vec(glove_file, word2vec_file)\n",
        "    print(\"Loading Glove vectors...\")\n",
        "    word_vectors = KeyedVectors.load_word2vec_format(word2vec_file)\n",
        "\n",
        "#     with open(\"model_glove_300.pkl\", 'rb') as handle:\n",
        "#         word_vectors = pickle.load(handle)\n",
        "        \n",
        "    word_vec_list = list()\n",
        "    for _, word in sorted(reversed_dict.items()):\n",
        "        try:\n",
        "            word_vec = word_vectors.word_vec(word)\n",
        "        except KeyError:\n",
        "            word_vec = np.zeros([embedding_size], dtype=np.float32)\n",
        "\n",
        "        word_vec_list.append(word_vec)\n",
        "\n",
        "    # Assign random vector to <s>, </s> token\n",
        "    word_vec_list[2] = np.random.normal(0, 1, embedding_size)\n",
        "    word_vec_list[3] = np.random.normal(0, 1, embedding_size)\n",
        "\n",
        "    return np.array(word_vec_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qI3AJND0mb0r"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X1B9ZgJqmsni"
      },
      "source": [
        "\n",
        "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/prep_data.py\n",
        "\n",
        "1.   Word Embedding : Used [Glove pre-trained vectors](https://nlp.stanford.edu/projects/glove/ ) to initialize word embedding.  \n",
        "2.   Dataset :  Dataset is available at [harvardnlp/sent-summary](https://github.com/harvardnlp/sent-summary). Locate the summary.tar.gz file in project root directory.   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g2dpVeOqnJAh",
        "colab": {}
      },
      "source": [
        "import wget\n",
        "import os\n",
        "import tarfile\n",
        "import gzip\n",
        "import zipfile\n",
        "import argparse\n",
        "\n",
        "\n",
        "#parser = argparse.ArgumentParser()\n",
        "#parser.add_argument(\"--glove\", action=\"store_true\")\n",
        "#args = parser.parse_args()\n",
        "\n",
        "# Extract data file\n",
        "#with tarfile.open(default_path + \"train/summary.tar.gz\", \"r:gz\") as tar:\n",
        "#    tar.extractall()\n",
        "\n",
        "# with gzip.open(default_path + \"train/train.article.txt.gz\", \"rb\") as gz:\n",
        "#     with open(default_path + \"train/train.article.txt\", \"wb\") as out:\n",
        "#         out.write(gz.read())\n",
        "\n",
        "# with gzip.open(default_path + \"train/train.title.txt.gz\", \"rb\") as gz:\n",
        "#     with open(default_path + \"train/train.title.txt\", \"wb\") as out:\n",
        "#         out.write(gz.read())\n",
        "\n",
        "        \n",
        "# if args.glove:\n",
        "#    glove_dir = \"glove\"\n",
        "#    glove_url = \"https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\"\n",
        "\n",
        "#    if not os.path.exists(glove_dir):\n",
        "#        os.mkdir(glove_dir)\n",
        "\n",
        "#    # Download glove vector\n",
        "#    wget.download(glove_url, out=glove_dir)\n",
        "\n",
        "#    # Extract glove file\n",
        "#    with zipfile.ZipFile(os.path.join(\"glove\", \"glove.42B.300d.zip\"), \"r\") as z:\n",
        "#        z.extractall(glove_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F_r1e_f2nCKK"
      },
      "source": [
        "##Model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MABp2aWMoNKh"
      },
      "source": [
        "Encoder-Decoder model with attention mechanism.\n",
        "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/model.py\n",
        "\n",
        "1.   Encoder : Used LSTM cell with stack_bidirectional_dynamic_rnn.\n",
        "2.   Decoder : Used LSTM BasicDecoder for training, and BeamSearchDecoder for inference.\n",
        "3.   Attention Mechanism : Used BahdanauAttention with weight normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GKHwTTZg5eJa",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):\n",
        "        self.vocabulary_size = len(reversed_dict)\n",
        "        self.embedding_size = args.embedding_size\n",
        "        self.num_hidden = args.num_hidden\n",
        "        self.num_layers = args.num_layers\n",
        "        self.learning_rate = args.learning_rate\n",
        "        self.beam_width = args.beam_width\n",
        "        if not forward_only:\n",
        "            self.keep_prob = args.keep_prob\n",
        "        else:\n",
        "            self.keep_prob = 1.0\n",
        "        self.cell = tf.nn.rnn_cell.BasicLSTMCell\n",
        "        with tf.variable_scope(\"decoder/projection\"):\n",
        "            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
        "\n",
        "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
        "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
        "        self.X_len = tf.placeholder(tf.int32, [None])\n",
        "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
        "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
        "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
        "        self.global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "        with tf.name_scope(\"embedding\"):\n",
        "            if not forward_only and args.glove:\n",
        "                init_embeddings = tf.constant(get_init_embedding(reversed_dict, self.embedding_size), dtype=tf.float32)\n",
        "            else:\n",
        "                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
        "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
        "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n",
        "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n",
        "\n",
        "        with tf.name_scope(\"encoder\"):\n",
        "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
        "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
        "            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]\n",
        "            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]\n",
        "\n",
        "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
        "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
        "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
        "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
        "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
        "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
        "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
        "\n",
        "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
        "            decoder_cell = self.cell(self.num_hidden * 2)\n",
        "\n",
        "            if not forward_only:\n",
        "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
        "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
        "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
        "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
        "                                                                   attention_layer_size=self.num_hidden * 2)\n",
        "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
        "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
        "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
        "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
        "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
        "                self.decoder_output = outputs.rnn_output\n",
        "                self.logits = tf.transpose(\n",
        "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
        "                self.logits_reshape = tf.concat(\n",
        "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
        "            else:\n",
        "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
        "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
        "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
        "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
        "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
        "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
        "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
        "                                                                   attention_layer_size=self.num_hidden * 2)\n",
        "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
        "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
        "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "                    cell=decoder_cell,\n",
        "                    embedding=self.embeddings,\n",
        "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
        "                    end_token=tf.constant(3),\n",
        "                    initial_state=initial_state,\n",
        "                    beam_width=self.beam_width,\n",
        "                    output_layer=self.projection_layer\n",
        "                )\n",
        "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
        "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
        "\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            if not forward_only:\n",
        "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
        "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
        "                self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))\n",
        "\n",
        "                params = tf.trainable_variables()\n",
        "                gradients = tf.gradients(self.loss, params)\n",
        "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x_xON6b4nS8x"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ak3tIhETnaIJ"
      },
      "source": [
        "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/train.py\n",
        "\n",
        "Use train/train.article.txt and train/train.title.txt for training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vq9ZA0hFnUZJ",
        "outputId": "09a7c605-6e9b-46d0-e5c7-49149e0ef6ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1499
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "import time\n",
        "start = time.perf_counter()\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import pickle\n",
        "import os\n",
        "#from model import Model\n",
        "#from utils import build_dict, build_dataset, batch_iter\n",
        "\n",
        "# Uncomment next 2 lines to suppress error and Tensorflow info verbosity. Or change logging levels\n",
        "# tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "#def add_arguments(parser):\n",
        "#    parser.add_argument(\"--num_hidden\", type=int, default=150, help=\"Network size.\")\n",
        "#    parser.add_argument(\"--num_layers\", type=int, default=2, help=\"Network depth.\")\n",
        "#    parser.add_argument(\"--beam_width\", type=int, default=10, help=\"Beam width for beam search decoder.\")\n",
        "#    parser.add_argument(\"--glove\", action=\"store_true\", help=\"Use glove as initial word embedding.\")\n",
        "#    parser.add_argument(\"--embedding_size\", type=int, default=300, help=\"Word embedding size.\")\n",
        "#\n",
        "#    parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"Learning rate.\")\n",
        "#    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size.\")\n",
        "#    parser.add_argument(\"--num_epochs\", type=int, default=10, help=\"Number of epochs.\")\n",
        "#    parser.add_argument(\"--keep_prob\", type=float, default=0.8, help=\"Dropout keep prob.\")\n",
        "#\n",
        "#    parser.add_argument(\"--toy\", action=\"store_true\", help=\"Use only 50K samples of data\")\n",
        "#\n",
        "#    parser.add_argument(\"--with_model\", action=\"store_true\", help=\"Continue from previously saved model\")\n",
        "\n",
        "class args:\n",
        "    pass\n",
        "  \n",
        "args.num_hidden=150\n",
        "args.num_layers=2\n",
        "args.beam_width=10\n",
        "args.glove=\"store_true\"\n",
        "args.embedding_size=300\n",
        "\n",
        "args.learning_rate=1e-3\n",
        "args.batch_size=64\n",
        "args.num_epochs=10\n",
        "args.keep_prob = 0.8\n",
        "\n",
        "args.toy=False #\"store_true\"\n",
        "\n",
        "args.with_model=\"store_true\"\n",
        "\n",
        "\n",
        "#parser = argparse.ArgumentParser()\n",
        "#add_arguments(parser)\n",
        "#args = parser.parse_args()\n",
        "#with open(\"args.pickle\", \"wb\") as f:\n",
        "#    pickle.dump(args, f)\n",
        "\n",
        "if not os.path.exists(\"saved_model\"):\n",
        "    os.mkdir(\"saved_model\")\n",
        "else:\n",
        "    #if args.with_model:\n",
        "        old_model_checkpoint_path = open('saved_model/checkpoint', 'r')\n",
        "        old_model_checkpoint_path = \"\".join([\"saved_model/\",old_model_checkpoint_path.read().splitlines()[0].split('\"')[1] ])\n",
        "\n",
        "\n",
        "print(\"Building dictionary...\")\n",
        "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", args.toy)\n",
        "print(\"Loading training dataset...\")\n",
        "train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, args.toy)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    model = Model(reversed_dict, article_max_len, summary_max_len, args)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    if 'old_model_checkpoint_path' in globals():\n",
        "        print(\"Continuing from previous trained model:\" , old_model_checkpoint_path , \"...\")\n",
        "        saver.restore(sess, old_model_checkpoint_path )\n",
        "\n",
        "    batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\n",
        "    num_batches_per_epoch = (len(train_x) - 1) // args.batch_size + 1\n",
        "\n",
        "    print(\"\\nIteration starts.\")\n",
        "    print(\"Number of batches per epoch :\", num_batches_per_epoch)\n",
        "    for batch_x, batch_y in batches:\n",
        "        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n",
        "        batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), batch_y))\n",
        "        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n",
        "        batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], batch_y))\n",
        "\n",
        "        batch_decoder_input = list(\n",
        "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\n",
        "        batch_decoder_output = list(\n",
        "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\n",
        "\n",
        "        train_feed_dict = {\n",
        "            model.batch_size: len(batch_x),\n",
        "            model.X: batch_x,\n",
        "            model.X_len: batch_x_len,\n",
        "            model.decoder_input: batch_decoder_input,\n",
        "            model.decoder_len: batch_decoder_len,\n",
        "            model.decoder_target: batch_decoder_output\n",
        "        }\n",
        "\n",
        "        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
        "\n",
        "        if step % 1000 == 0:\n",
        "            print(\"step {0}: loss = {1}\".format(step, loss))\n",
        "\n",
        "        if step % num_batches_per_epoch == 0:\n",
        "            hours, rem = divmod(time.perf_counter() - start, 3600)\n",
        "            minutes, seconds = divmod(rem, 60)\n",
        "            saver.save(sess, \"saved_model/model.ckpt\", global_step=step)\n",
        "            print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\n",
        "            \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) , \"\\n\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building dictionary...\n",
            "Loading training dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading Glove vectors...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0614 05:16:01.223193 139630703044480 deprecation.py:323] From <ipython-input-13-3e3c149b757b>:40: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0614 05:16:01.239972 139630703044480 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/rnn/python/ops/rnn.py:239: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "W0614 05:16:01.243323 139630703044480 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0614 05:16:01.363990 139630703044480 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0614 05:16:01.379861 139630703044480 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0614 05:16:01.729996 139630703044480 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0614 05:16:03.575754 139630703044480 deprecation.py:323] From <ipython-input-13-3e3c149b757b>:101: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration starts.\n",
            "Number of batches per epoch : 3125\n",
            "step 1000: loss = 48.56650161743164\n",
            "step 2000: loss = 37.45339584350586\n",
            "step 3000: loss = 35.38955307006836\n",
            " Epoch 1: Model is saved. Elapsed: 00:25:17.55 \n",
            "\n",
            "step 4000: loss = 33.68756866455078\n",
            "step 5000: loss = 35.29840087890625\n",
            "step 6000: loss = 27.300079345703125\n",
            " Epoch 2: Model is saved. Elapsed: 00:38:01.38 \n",
            "\n",
            "step 7000: loss = 30.049083709716797\n",
            "step 8000: loss = 22.10747528076172\n",
            "step 9000: loss = 21.429994583129883\n",
            " Epoch 3: Model is saved. Elapsed: 00:50:45.61 \n",
            "\n",
            "step 10000: loss = 19.815256118774414\n",
            "step 11000: loss = 17.53327751159668\n",
            "step 12000: loss = 13.953457832336426\n",
            " Epoch 4: Model is saved. Elapsed: 01:03:29.58 \n",
            "\n",
            "step 13000: loss = 19.24254608154297\n",
            "step 14000: loss = 12.913619041442871\n",
            "step 15000: loss = 17.103111267089844\n",
            " Epoch 5: Model is saved. Elapsed: 01:16:06.93 \n",
            "\n",
            "step 16000: loss = 16.89795684814453\n",
            "step 17000: loss = 17.54222297668457\n",
            "step 18000: loss = 15.565654754638672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0614 06:32:16.249577 139630703044480 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Epoch 6: Model is saved. Elapsed: 01:28:41.07 \n",
            "\n",
            "step 19000: loss = 15.311954498291016\n",
            "step 20000: loss = 13.111289978027344\n",
            "step 21000: loss = 10.104683876037598\n",
            " Epoch 7: Model is saved. Elapsed: 01:41:11.20 \n",
            "\n",
            "step 22000: loss = 9.558016777038574\n",
            "step 23000: loss = 10.83720588684082\n",
            "step 24000: loss = 8.748115539550781\n",
            "step 25000: loss = 14.962991714477539\n",
            " Epoch 8: Model is saved. Elapsed: 01:53:37.65 \n",
            "\n",
            "step 26000: loss = 15.608682632446289\n",
            "step 27000: loss = 9.522758483886719\n",
            "step 28000: loss = 11.305787086486816\n",
            " Epoch 9: Model is saved. Elapsed: 02:06:03.44 \n",
            "\n",
            "step 29000: loss = 10.565549850463867\n",
            "step 30000: loss = 16.50485610961914\n",
            "step 31000: loss = 8.069765090942383\n",
            " Epoch 10: Model is saved. Elapsed: 02:18:29.85 \n",
            "\n",
            "CPU times: user 2h 50min 24s, sys: 18min 53s, total: 3h 9min 17s\n",
            "Wall time: 2h 18min 33s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TbbtwLe7njGS"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iCBIdwESnlhZ"
      },
      "source": [
        "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/test.py\n",
        "\n",
        "Generate summary of each article in sumdata/train/valid.article.filter.txt by"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1IUJ0dpon1Hi",
        "outputId": "05ea45b0-300c-414e-a3bc-37a36b73c9da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "#from model import Model\n",
        "#from utils import build_dict, build_dataset, batch_iter\n",
        "\n",
        "\n",
        "#with open(\"args.pickle\", \"rb\") as f:\n",
        "#    args = pickle.load(f)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "class args:\n",
        "    pass\n",
        "  \n",
        "args.num_hidden=150\n",
        "args.num_layers=2\n",
        "args.beam_width=10\n",
        "args.glove=\"store_true\"\n",
        "args.embedding_size=300\n",
        "\n",
        "args.learning_rate=1e-3\n",
        "args.batch_size=64\n",
        "args.num_epochs=10\n",
        "args.keep_prob = 0.8\n",
        "\n",
        "args.toy=True\n",
        "\n",
        "args.with_model=\"store_true\"\n",
        "\n",
        "\n",
        "\n",
        "print(\"Loading dictionary...\")\n",
        "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
        "print(\"Loading validation dataset...\")\n",
        "valid_x = build_dataset(\"valid\", word_dict, article_max_len, summary_max_len, args.toy)\n",
        "valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
        "print(\"Loading article and reference...\")\n",
        "article = get_text_list(valid_article_path, args.toy)\n",
        "reference = get_text_list(valid_title_path, args.toy)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    print(\"Loading saved model...\")\n",
        "    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    ckpt = tf.train.get_checkpoint_state(\"saved_model/\")\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "    batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
        "\n",
        "    print(\"Writing summaries to 'result.txt'...\")\n",
        "    for batch_x, _ in batches:\n",
        "        batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
        "\n",
        "        valid_feed_dict = {\n",
        "            model.batch_size: len(batch_x),\n",
        "            model.X: batch_x,\n",
        "            model.X_len: batch_x_len,\n",
        "        }\n",
        "\n",
        "        prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
        "        prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
        "        summary_array = []\n",
        "        with open(\"result.txt\", \"a\") as f:\n",
        "            for line in prediction_output:\n",
        "                summary = list()\n",
        "                for word in line:\n",
        "                    if word == \"</s>\":\n",
        "                        break\n",
        "                    if word not in summary:\n",
        "                        summary.append(word)\n",
        "                summary_array.append(\" \".join(summary))\n",
        "                #print(\" \".join(summary), file=f)\n",
        "\n",
        "    print('Summaries have been generated')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dictionary...\n",
            "Loading validation dataset...\n",
            "Loading article and reference...\n",
            "Loading saved model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0614 07:22:19.956315 139630703044480 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py:985: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0614 07:22:20.185108 139630703044480 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing summaries to 'result.txt'...\n",
            "Summaries have been generated\n",
            "CPU times: user 4.72 s, sys: 173 ms, total: 4.89 s\n",
            "Wall time: 14.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K-Kj9e5M9lBr"
      },
      "source": [
        "## Evaluate & write output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "svL3XWnXaTsW"
      },
      "source": [
        "for comparing (good resources)\n",
        "\n",
        "1.  [thunlp]( https://github.com/thunlp/TensorFlow-Summarization)     works with duc2003\n",
        "2.  [textsum](https://github.com/tensorflow/models/tree/master/research/textsum )   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tyi0SXZL0GnH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "4b17dbd4-aa7a-4c78-d58f-6d996a7abf86"
      },
      "source": [
        "!pip install sumeval\n",
        "!python -m spacy download en"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sumeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/d0/23c9a37253044d478efede0770aefaea52a5b75911da56c3ac0aca894d2a/sumeval-0.2.0.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: plac>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from sumeval) (0.9.6)\n",
            "Collecting sacrebleu>=1.3.2 (from sumeval)\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/e4/0c186661604e70f4be57d44510115e63818189900cb0262bc5cd541bab3e/sacrebleu-1.3.5.tar.gz\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.3.2->sumeval) (3.6.6)\n",
            "Building wheels for collected packages: sumeval, sacrebleu\n",
            "  Building wheel for sumeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9b/50/fda7087af57d2efde31d4a85cecda98e683ed58eee07fbff2b\n",
            "  Building wheel for sacrebleu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/f5/21/da/3e3dea18eb53af3b56d99b4b6ae50c91c89c8a7f422d676ee1\n",
            "Successfully built sumeval sacrebleu\n",
            "Installing collected packages: sacrebleu, sumeval\n",
            "Successfully installed sacrebleu-1.3.5 sumeval-0.2.0\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x6nERCu29oPS",
        "colab": {}
      },
      "source": [
        "#https://github.com/chakki-works/sumeval\n",
        "#https://github.com/Tian312/awesome-text-summarization\n",
        "\n",
        "from sumeval.metrics.rouge import RougeCalculator\n",
        "from sumeval.metrics.bleu import BLEUCalculator\n",
        "\n",
        "def eval_rouges(refrence_summary,model_summary):\n",
        "    #refrence_summary = \"tokyo shares close up #.## percent\"\n",
        "    #model_summary = \"tokyo stocks close up # percent to fresh record high\"\n",
        "\n",
        "    rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
        "\n",
        "    rouge_1 = rouge.rouge_n(\n",
        "                summary=model_summary,\n",
        "                references=refrence_summary,\n",
        "                n=1)\n",
        "\n",
        "    rouge_2 = rouge.rouge_n(\n",
        "                summary=model_summary,\n",
        "                references=[refrence_summary],\n",
        "                n=2)\n",
        "    \n",
        "    rouge_l = rouge.rouge_l(\n",
        "                summary=model_summary,\n",
        "                references=[refrence_summary])\n",
        "    \n",
        "    # You need spaCy to calculate ROUGE-BE\n",
        "    \n",
        "    rouge_be = rouge.rouge_be(\n",
        "                summary=model_summary,\n",
        "                references=[refrence_summary])\n",
        "\n",
        "    bleu = BLEUCalculator()\n",
        "    bleu_score = bleu.bleu( summary=model_summary,\n",
        "                        references=[refrence_summary])\n",
        "\n",
        "    #print(\"ROUGE-1: {}, ROUGE-2: {}, ROUGE-L: {}, ROUGE-BE: {}\".format(\n",
        "    #    rouge_1, rouge_2, rouge_l, rouge_be\n",
        "    #).replace(\", \", \"\\n\"))\n",
        "    \n",
        "    return rouge_1, rouge_2,rouge_l,rouge_be,bleu_score\n",
        "  \n",
        "#rouge_1, rouge_2,rouge_l,rouge_be = eval_rouges( \"tokyo shares close up #.## percent\",\n",
        "#                                                \"tokyo stocks close up # percent to fresh record high\")\n",
        "#\n",
        "#print(\"ROUGE-1: {}, ROUGE-2: {}, ROUGE-L: {}, ROUGE-BE: {}\".format(\n",
        "#        rouge_1, rouge_2, rouge_l, rouge_be\n",
        "#    ).replace(\", \", \"\\n\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EyKcd5ffKEBJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5576
        },
        "outputId": "87ec191d-15ba-4f86-9b36-16486c1110fd"
      },
      "source": [
        "#https://pymotw.com/2/xml/etree/ElementTree/create.html\n",
        "\n",
        "bleu_arr = []\n",
        "rouge_1_arr  = []\n",
        "rouge_2_arr  = []\n",
        "rouge_L_arr  = []\n",
        "rouge_be_arr = []\n",
        "\n",
        "from xml.etree import ElementTree\n",
        "from xml.dom import minidom\n",
        "from functools import reduce\n",
        "\n",
        "def prettify(elem):\n",
        "    \"\"\"Return a pretty-printed XML string for the Element.\n",
        "    \"\"\"\n",
        "    rough_string = ElementTree.tostring(elem, 'utf-8')\n",
        "    reparsed = minidom.parseString(rough_string)\n",
        "    return reparsed.toprettyxml(indent=\"  \")\n",
        "  \n",
        "from xml.etree.ElementTree import Element, SubElement, Comment\n",
        "\n",
        "top = Element('ZakSum')\n",
        "\n",
        "comment = Comment('Generated by Amr Zaki')\n",
        "top.append(comment)\n",
        "\n",
        "i=0\n",
        "for summ in summary_array:\n",
        "  example = SubElement(top, 'example')\n",
        "  article_element   = SubElement(example, 'article')\n",
        "  article_element.text = article[i]\n",
        "  \n",
        "  reference_element = SubElement(example, 'reference')\n",
        "  reference_element.text = reference[i]\n",
        "  \n",
        "  summary_element   = SubElement(example, 'summary')\n",
        "  summary_element.text = summ\n",
        "\n",
        "  rouge_1, rouge_2,rouge_L,rouge_be,bleu_score = eval_rouges(reference[i],summ )\n",
        "  \n",
        "  eval_element = SubElement(example, 'eval')\n",
        "  bleu_score_element = SubElement(eval_element,'BLEU', {'score':str(bleu_score)})\n",
        "  ROUGE_1_element  = SubElement(eval_element, 'ROUGE_1' , {'score':str(rouge_1)})\n",
        "  ROUGE_2_element  = SubElement(eval_element, 'ROUGE_2' , {'score':str(rouge_2)})\n",
        "  ROUGE_L_element  = SubElement(eval_element, 'ROUGE_l' , {'score':str(rouge_L)})\n",
        "  ROUGE_be_element  = SubElement(eval_element,'ROUGE_be', {'score':str(rouge_be)})\n",
        "  \n",
        "  bleu_arr.append(bleu_score) \n",
        "  rouge_1_arr.append(rouge_1) \n",
        "  rouge_2_arr.append(rouge_2) \n",
        "  rouge_L_arr.append(rouge_L) \n",
        "  rouge_be_arr.append(rouge_be) \n",
        "\n",
        "  i+=1\n",
        "\n",
        "top.set('bleu', str(reduce(lambda x, y: x + y,  bleu_arr) / len(bleu_arr)))\n",
        "top.set('rouge_1', str(reduce(lambda x, y: x + y,  rouge_1_arr) / len(rouge_1_arr)))\n",
        "top.set('rouge_2', str(reduce(lambda x, y: x + y,  rouge_2_arr) / len(rouge_2_arr)))\n",
        "top.set('rouge_L', str(reduce(lambda x, y: x + y,  rouge_L_arr) / len(rouge_L_arr)))\n",
        "top.set('rouge_be', str(reduce(lambda x, y: x + y, rouge_be_arr) / len(rouge_be_arr)))\n",
        "\n",
        "with open(\"result_valid.xml\", \"w+\") as f:\n",
        "  print(prettify(top), file=f)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a.kwan=(nsubj)=>withdraws\n",
            "<BasicElement: kwan-[nsubj]->withdraw>\n",
            "b.olympic=(amod)=>skating\n",
            "a.event=(dobj)=>withdraws\n",
            "<BasicElement: event-[dobj]->withdraw>\n",
            "a.injury=(nsubj)=>leaves\n",
            "<BasicElement: injury-[nsubj]->leave>\n",
            "a.kwan=(dobj)=>leaves\n",
            "<BasicElement: kwan-[dobj]->leave>\n",
            "b.olympic=(amod)=>hopes\n",
            "a.leaders=(nsubj)=>lash\n",
            "<BasicElement: leaders-[nsubj]->lash>\n",
            "b.illegal=(amod)=>immigrants\n",
            "a.attacks=(nsubj)=>tough\n",
            "<BasicElement: attacks-[nsubj]->tough>\n",
            "a.law=(dobj)=>tough\n",
            "<BasicElement: law-[dobj]->tough>\n",
            "a.gm=(mark)=>fall\n",
            "<BasicElement: gm-[mark]->fall>\n",
            "a.sales=(nsubj)=>fall\n",
            "<BasicElement: sales-[nsubj]->fall>\n",
            "a.percent=(dobj)=>fall\n",
            "<BasicElement: percent-[dobj]->fall>\n",
            "a.sales=(nsubj)=>fall\n",
            "<BasicElement: sales-[nsubj]->fall>\n",
            "a.percent=(dobj)=>fall\n",
            "<BasicElement: percent-[dobj]->fall>\n",
            "a.thousands=(nummod)=>protest\n",
            "<BasicElement: thousands-[nummod]->protest>\n",
            "a.thousands=(nsubj)=>celebrate\n",
            "<BasicElement: thousands-[nsubj]->celebrate>\n",
            "a.cup=(compound)=>slalom\n",
            "<BasicElement: cup-[compound]->slalom>\n",
            "b.laura=(nsubj)=>bush\n",
            "b.first=(amod)=>lady\n",
            "b.represent=(advcl)=>bush\n",
            "b.laura=(amod)=>bush\n",
            "b.attend=(advcl)=>rice\n",
            "a.sirleaf=(dobj)=>attend\n",
            "<BasicElement: sirleaf-[dobj]->attend>\n",
            "b.top=(amod)=>lobbyist\n",
            "b.republican=(amod)=>lobbyist\n",
            "b.guilty=(amod)=>pleads\n",
            "b.somali=(amod)=>parliament\n",
            "a.parliament=(nsubj)=>agree\n",
            "<BasicElement: parliament-[nsubj]->agree>\n",
            "a.compromise=(dobj)=>agree\n",
            "<BasicElement: compromise-[dobj]->agree>\n",
            "b.portuguese=(amod)=>workers\n",
            "a.workers=(nsubj)=>call\n",
            "<BasicElement: workers-[nsubj]->call>\n",
            "b.possible=(amod)=>action\n",
            "b.portuguese=(amod)=>strike\n",
            "a.strike=(nsubj)=>ground\n",
            "<BasicElement: strike-[nsubj]->ground>\n",
            "a.flights=(dobj)=>ground\n",
            "<BasicElement: flights-[dobj]->ground>\n",
            "a.signs=(pobj)=>eib\n",
            "<BasicElement: signs-[pobj]->eib>\n",
            "b.portuguese=(amod)=>midfielder\n",
            "b.renews=(compound)=>partnership\n",
            "a.clooney=(nsubj)=>try\n",
            "<BasicElement: clooney-[nsubj]->try>\n",
            "a.virus=(dobj)=>mars\n",
            "<BasicElement: virus-[dobj]->mars>\n",
            "a.canada=(nsubj)=>advises\n",
            "<BasicElement: canada-[nsubj]->advise>\n",
            "a.nationals=(dobj)=>advises\n",
            "<BasicElement: nationals-[dobj]->advise>\n",
            "b.nepal=(amod)=>travel\n",
            "a.travel=(dobj)=>avoid\n",
            "<BasicElement: travel-[dobj]->avoid>\n",
            "a.canada=(nsubj)=>recommends\n",
            "<BasicElement: canada-[nsubj]->recommend>\n",
            "a.travel=(dobj)=>avoiding\n",
            "<BasicElement: travel-[dobj]->avoid>\n",
            "a.sales=(nsubj)=>hit\n",
            "<BasicElement: sales-[nsubj]->hit>\n",
            "a.percent=(dobj)=>hit\n",
            "<BasicElement: percent-[dobj]->hit>\n",
            "a.executive=(nsubj)=>sees\n",
            "<BasicElement: executive-[nsubj]->see>\n",
            "a.sales=(dobj)=>weaker\n",
            "<BasicElement: sales-[dobj]->weak>\n",
            "a.china=(dobj)=>welcomes\n",
            "<BasicElement: china-[dobj]->welcome>\n",
            "b.jailed=(amod)=>journalist\n",
            "b.chinese=(amod)=>journalist\n",
            "a.journalist=(dobj)=>welcomes\n",
            "<BasicElement: journalist-[dobj]->welcome>\n",
            "a.concerns=(dobj)=>highlights\n",
            "<BasicElement: concerns-[dobj]->highlight>\n",
            "a.bankers=(nsubj)=>admit\n",
            "<BasicElement: bankers-[nsubj]->admit>\n",
            "a.theft=(dobj)=>admit\n",
            "<BasicElement: theft-[dobj]->admit>\n",
            "b.israeli=(amod)=>pm\n",
            "b.hospitalized=(acl)=>pm\n",
            "b.unwell=(amod)=>sharon\n",
            "a.sharon=(nsubj)=>admitted\n",
            "<BasicElement: sharon-[nsubj]->admit>\n",
            "a.sales=(nsubj)=>hit\n",
            "<BasicElement: sales-[nsubj]->hit>\n",
            "b.hybrid=(amod)=>vehicle\n",
            "a.sales=(nsubj)=>expect\n",
            "<BasicElement: sales-[nsubj]->expect>\n",
            "b.headed=(acl)=>seo\n",
            "b.teenage=(amod)=>teenager\n",
            "b.asthma=(xcomp)=>confesses\n",
            "a.cancer=(dobj)=>asthma\n",
            "<BasicElement: cancer-[dobj]->asthma>\n",
            "a.hollywood=(nsubj)=>starlet\n",
            "<BasicElement: hollywood-[nsubj]->starlet>\n",
            "b.starlet=(compound)=>lindsay\n",
            "a.lindsay=(nsubj)=>admits\n",
            "<BasicElement: lindsay-[nsubj]->admit>\n",
            "a.battle=(dobj)=>admits\n",
            "<BasicElement: battle-[dobj]->admit>\n",
            "b.iraqi=(amod)=>civilians\n",
            "a.soldiers=(nsubj)=>act\n",
            "<BasicElement: soldiers-[nsubj]->act>\n",
            "a.civilians=(dobj)=>protect\n",
            "<BasicElement: civilians-[dobj]->protect>\n",
            "a.envoy=(nsubj)=>quits\n",
            "<BasicElement: envoy-[nsubj]->quit>\n",
            "b.north=(compound)=>korea\n",
            "a.korea=(dobj)=>quits\n",
            "<BasicElement: korea-[dobj]->quit>\n",
            "b.special=(amod)=>envoy\n",
            "b.korean=(relcl)=>envoy\n",
            "b.nuclear=(amod)=>talks\n",
            "a.talks=(dobj)=>korean\n",
            "<BasicElement: talks-[dobj]->korean>\n",
            "a.summary=(dobj)=>afp\n",
            "<BasicElement: summary-[dobj]->afp>\n",
            "b.positive=(amod)=>test\n",
            "a.envoy=(nsubj)=>urges\n",
            "<BasicElement: envoy-[nsubj]->urge>\n",
            "b.greater=(amod)=>support\n",
            "a.support=(dobj)=>urges\n",
            "<BasicElement: support-[dobj]->urge>\n",
            "a.britain=(nsubj)=>urges\n",
            "<BasicElement: britain-[nsubj]->urge>\n",
            "b.stronger=(amod)=>support\n",
            "b.international=(amod)=>support\n",
            "a.support=(dobj)=>urges\n",
            "<BasicElement: support-[dobj]->urge>\n",
            "a.australia=(nsubj)=>calls\n",
            "<BasicElement: australia-[nsubj]->call>\n",
            "a.australia=(nsubj)=>backs\n",
            "<BasicElement: australia-[nsubj]->back>\n",
            "a.others=(dobj)=>brazil\n",
            "<BasicElement: others-[dobj]->brazil>\n",
            "b.egyptian=(amod)=>guards\n",
            "b.killed=(acl)=>guards\n",
            "b.egyptian=(amod)=>guards\n",
            "a.guards=(nsubj)=>killed\n",
            "<BasicElement: guards-[nsubj]->kill>\n",
            "a.conservatives=(nsubj)=>leave\n",
            "<BasicElement: conservatives-[nsubj]->leave>\n",
            "a.opposition=(dobj)=>leave\n",
            "<BasicElement: opposition-[dobj]->leave>\n",
            "a.conservatives=(nsubj)=>gain\n",
            "<BasicElement: conservatives-[nsubj]->gain>\n",
            "a.momentum=(dobj)=>gain\n",
            "<BasicElement: momentum-[dobj]->gain>\n",
            "b.ruling=(amod)=>liberals\n",
            "b.dutch=(amod)=>banks\n",
            "a.banks=(nsubj)=>hit\n",
            "<BasicElement: banks-[nsubj]->hit>\n",
            "b.abn=(nummod)=>amro\n",
            "b.hit=(acl)=>amro\n",
            "b.unk=(nmod)=>bills\n",
            "a.bills=(nsubj)=>sack\n",
            "<BasicElement: bills-[nsubj]->sack>\n",
            "b.general=(amod)=>manager\n",
            "a.manager=(dobj)=>sack\n",
            "<BasicElement: manager-[dobj]->sack>\n",
            "a.bills=(nsubj)=>shake\n",
            "<BasicElement: bills-[nsubj]->shake>\n",
            "b.front=(amod)=>office\n",
            "a.office=(dobj)=>shake\n",
            "<BasicElement: office-[dobj]->shake>\n",
            "a.fright=(advcl)=>beat\n",
            "<BasicElement: fright-[advcl]->beat>\n",
            "a.woes=(dobj)=>spurs\n",
            "<BasicElement: woes-[dobj]->spur>\n",
            "a.dollar=(nsubj)=>hits\n",
            "<BasicElement: dollar-[nsubj]->hit>\n",
            "a.low=(acomp)=>hits\n",
            "<BasicElement: low-[acomp]->hit>\n",
            "a.outlook=(nsubj)=>sends\n",
            "<BasicElement: outlook-[nsubj]->send>\n",
            "a.dollar=(dobj)=>sends\n",
            "<BasicElement: dollar-[dobj]->send>\n",
            "b.oscars=(compound)=>winner\n",
            "b.good=(amod)=>night\n",
            "b.top=(amod)=>groups\n",
            "a.groups=(nsubj)=>pick\n",
            "<BasicElement: groups-[nsubj]->pick>\n",
            "a.nominees=(dobj)=>pick\n",
            "<BasicElement: nominees-[dobj]->pick>\n",
            "a.oscars=(nsubj)=>loom\n",
            "<BasicElement: oscars-[nsubj]->loom>\n",
            "b.ink=(nmod)=>burnitz\n",
            "a.bush=(nsubj)=>sees\n",
            "<BasicElement: bush-[nsubj]->see>\n",
            "a.hope=(dobj)=>sees\n",
            "<BasicElement: hope-[dobj]->see>\n",
            "a.bush=(nsubj)=>says\n",
            "<BasicElement: bush-[nsubj]->say>\n",
            "a.concern=(dobj)=>shares\n",
            "<BasicElement: concern-[dobj]->share>\n",
            "b.unk=(amod)=>minister\n",
            "b.prime=(amod)=>minister\n",
            "a.minister=(nsubj)=>resigns\n",
            "<BasicElement: minister-[nsubj]->resign>\n",
            "b.burkina=(compound)=>faso\n",
            "b.prime=(amod)=>minister\n",
            "a.minister=(nsubj)=>resigns\n",
            "<BasicElement: minister-[nsubj]->resign>\n",
            "a.lawyer=(nsubj)=>seeks\n",
            "<BasicElement: lawyer-[nsubj]->seek>\n",
            "a.hilton=(dobj)=>stop\n",
            "<BasicElement: hilton-[dobj]->stop>\n",
            "a.man=(nsubj)=>seeks\n",
            "<BasicElement: man-[nsubj]->seek>\n",
            "a.order=(dobj)=>seeks\n",
            "<BasicElement: order-[dobj]->seek>\n",
            "a.session=(dobj)=>hold\n",
            "<BasicElement: session-[dobj]->hold>\n",
            "b.great=(amod)=>region\n",
            "b.hold=(relcl)=>council\n",
            "b.ministerial=(amod)=>session\n",
            "a.session=(dobj)=>hold\n",
            "<BasicElement: session-[dobj]->hold>\n",
            "b.great=(amod)=>lakes\n",
            "b.israeli=(amod)=>leaders\n",
            "b.political=(amod)=>leaders\n",
            "b.israeli=(amod)=>leaders\n",
            "a.leaders=(nsubj)=>unite\n",
            "<BasicElement: leaders-[nsubj]->unite>\n",
            "b.ailing=(amod)=>sharon\n",
            "b.undergoes=(amod)=>operation\n",
            "a.deportation=(dobj)=>deliver\n",
            "<BasicElement: deportation-[dobj]->deliver>\n",
            "b.human=(amod)=>victims\n",
            "a.victims=(nsubj)=>get\n",
            "<BasicElement: victims-[nsubj]->get>\n",
            "a.record=(npadvmod)=>breaking\n",
            "<BasicElement: record-[npadvmod]->break>\n",
            "b.breaking=(amod)=>nadal\n",
            "b.international=(amod)=>sydney\n",
            "a.campbell=(nsubj)=>waives\n",
            "<BasicElement: campbell-[nsubj]->waive>\n",
            "a.duties=(dobj)=>waives\n",
            "<BasicElement: duties-[dobj]->waive>\n",
            "b.odd=(amod)=>man\n",
            "a.devils=(nsubj)=>welcome\n",
            "<BasicElement: devils-[nsubj]->welcome>\n",
            "a.elias=(dobj)=>welcome\n",
            "<BasicElement: elias-[dobj]->welcome>\n",
            "b.israeli=(amod)=>minister\n",
            "b.meet=(relcl)=>minister\n",
            "a.wednesday=(npadvmod)=>meet\n",
            "<BasicElement: wednesday-[npadvmod]->meet>\n",
            "a.schedule=(dobj)=>afp\n",
            "<BasicElement: schedule-[dobj]->afp>\n",
            "b.nick=(compound)=>nolte\n",
            "b.unk=(punct)=>ends\n",
            "a.summary=(dobj)=>afp\n",
            "<BasicElement: summary-[dobj]->afp>\n",
            "a.confidence=(compound)=>tops\n",
            "<BasicElement: confidence-[compound]->top>\n",
            "b.tops=(conj)=>skorea\n",
            "a.benchmark=(dobj)=>tops\n",
            "<BasicElement: benchmark-[dobj]->top>\n",
            "a.charlie=(nsubj)=>tied\n",
            "<BasicElement: charlie-[nsubj]->tie>\n",
            "a.differences=(dobj)=>patch\n",
            "<BasicElement: differences-[dobj]->patch>\n",
            "a.richards=(compound)=>charlie\n",
            "<BasicElement: richards-[compound]->charlie>\n",
            "b.charlie=(compound)=>sheen\n",
            "a.sheen=(nsubj)=>push\n",
            "<BasicElement: sheen-[nsubj]->push>\n",
            "b.hong=(amod)=>kong\n",
            "a.gold=(nsubj)=>opens\n",
            "<BasicElement: gold-[nsubj]->open>\n",
            "b.hong=(amod)=>kong\n",
            "a.gold=(nsubj)=>opens\n",
            "<BasicElement: gold-[nsubj]->open>\n",
            "a.shares=(nsubj)=>rise\n",
            "<BasicElement: shares-[nsubj]->rise>\n",
            "a.shares=(nsubj)=>rise\n",
            "<BasicElement: shares-[nsubj]->rise>\n",
            "a.percent=(npadvmod)=>rise\n",
            "<BasicElement: percent-[npadvmod]->rise>\n",
            "a.police=(compound)=>fight\n",
            "<BasicElement: police-[compound]->fight>\n",
            "a.britain=(dobj)=>return\n",
            "<BasicElement: britain-[dobj]->return>\n",
            "b.british=(amod)=>police\n",
            "a.police=(nsubj)=>seek\n",
            "<BasicElement: police-[nsubj]->seek>\n",
            "a.moss=(dobj)=>arrest\n",
            "<BasicElement: moss-[dobj]->arrest>\n",
            "b.major=(amod)=>strikes\n",
            "b.israeli=(amod)=>strikes\n",
            "a.strikes=(nsubj)=>hits\n",
            "<BasicElement: strikes-[nsubj]->hit>\n",
            "a.sharon=(dobj)=>hits\n",
            "<BasicElement: sharon-[dobj]->hit>\n",
            "b.key=(amod)=>facts\n",
            "b.hemorrhagic=(amod)=>stroke\n",
            "b.hong=(amod)=>kong\n",
            "b.open=(amod)=>shares\n",
            "b.higher=(advmod)=>shares\n",
            "b.hong=(amod)=>kong\n",
            "b.open=(amod)=>shares\n",
            "b.higher=(advmod)=>shares\n",
            "a.worries=(nsubj)=>ease\n",
            "<BasicElement: worries-[nsubj]->ease>\n",
            "b.ease=(advcl)=>shares\n",
            "b.south=(amod)=>korea\n",
            "b.economic=(amod)=>relations\n",
            "b.korean=(amod)=>doubles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ihl49sb7bIRv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31c235c6-f02d-4329-9e68-ecabb60fd3d0"
      },
      "source": [
        "len(summary_array)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9Gdso5UgUC6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "14d5bfa1-9a45-45e9-cbfe-d4ad7f7123c5"
      },
      "source": [
        "!ls -lrt"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 6743596\n",
            "-rw-rw-r-- 1 root root 5025032584 Dec 22  2015 glove.42B.300d.txt\n",
            "-rw-r--r-- 1 root root 1877802108 Dec 22  2015 glove.42B.300d.zip\n",
            "-rw-r--r-- 1 root root    1236332 May 18 15:04 google-drive-ocamlfuse_0.7.4-0ubuntu1~ubuntu18.10.1_amd64.deb\n",
            "drwxr-xr-x 1 root root       4096 May 31 16:17 sample_data\n",
            "-rw-r--r-- 1 root root       2557 Jun 14 04:57 adc.json\n",
            "drwxr-xr-x 2 root root       4096 Jun 14 04:58 drive\n",
            "-rw-r--r-- 1 root root    1314472 Jun 14 05:05 word_dict.pickle\n",
            "drwxr-xr-x 2 root root       4096 Jun 14 07:22 saved_model\n",
            "-rw-r--r-- 1 root root          0 Jun 14 07:22 result.txt\n",
            "-rw-r--r-- 1 root root      28292 Jun 14 07:22 result_valid.xml\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}